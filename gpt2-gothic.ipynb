{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6541,"status":"ok","timestamp":1724763030950,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"58MTyEIH3ZTo","outputId":"6c20fbec-5a7b-49ca-83a2-d46a0588da9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]}],"source":["import math\n","import random\n","!pip install tiktoken\n","import tiktoken\n","import inspect\n","from dataclasses import dataclass\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# ---------------------------------------------\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","        # regularization\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","            .view(1, 1, config.block_size, config.block_size))\n","\n","    def forward(self, x):\n","        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n","        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n","        qkv = self.c_attn(x)\n","        q, k, v = qkv.split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        # attention (materializes the large (T*T) matrix for all the queries and keys)\n","        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n","        # att = F.softmax(att, dim=-1)\n","        # y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n","        self.gelu = nn.GELU(approximate='tanh')\n","        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 1024  # max sequence length\n","    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    # 'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","    # 'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","    # 'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","    # 'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","    n_layer: int = 24  # number of layers\n","    n_head: int = 16  # number of heads\n","    n_embd: int = 1024  # embedding dimension\n","\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # weight sharing scheme\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        # init params\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            std = 0.02\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer) ** -0.5\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","\n","\n","\n","    def forward(self, idx, targets=None):\n","        # idx is of shape (B, T)\n","        B, T = idx.size()\n","        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n","        # forward the token and position embeddings\n","        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n","        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n","        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n","        x = tok_emb + pos_emb\n","        # forward the blocks of the transformer\n","        for block in self.transformer.h:\n","            x = block(x)\n","        # forward the final layernorm and the classifier\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x)  # (B, T, vocab_size)\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","        return logits, loss\n","\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type):\n","        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        from transformers import GPT2LMHeadModel\n","        print(\"loading weights from pretrained gpt: %s\" % model_type)\n","\n","        # n_layer, n_head and n_embd are determined from model_type\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n","        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n","        # create a from-scratch initialized minGPT model\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n","\n","        # init a huggingface/transformers model\n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = model_hf.state_dict()\n","\n","        # copy while ensuring all of the parameters are aligned and match in names and shapes\n","        sd_keys_hf = sd_hf.keys()\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n","        # this means that we have to transpose these weights when we import them\n","        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                # special treatment for the Conv1D weights we need to transpose\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                # vanilla copy over the other parameters\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device):\n","        # start with all of the candidate parameters (that require grad)\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n","        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n","        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params} parameters\")\n","        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\")\n","\n","        # Create AdamW optimizer and use the fused version if it is available\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and 'cuda' in device\n","        print(f\"using fused AdamW: {use_fused}\")\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n","        return optimizer\n","\n","class DataLoaderLite:\n","    def __init__(self, B, T, val_split=0.2):\n","        self.B = B\n","        self.T = T\n","\n","        # Load the tokens and split them into training and validation sets\n","        with open('./gothic_novels_combined.txt', 'r') as f:\n","            text = f.read()\n","        print(f\"loaded {len(text)} characters\")\n","        enc = tiktoken.get_encoding('gpt2')\n","\n","        tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n","        len_tokens = len(tokens)\n","\n","        # Ensure tokens are stored on CPU\n","        self.tokens = torch.tensor(tokens, device='cpu')\n","        print(f\"loaded: {len(self.tokens)} tokens\")\n","\n","        # Split into training and validation buffers on CPU\n","        buffers = []\n","        current_position = 0\n","        while current_position + (B * T + 1) < len_tokens:\n","            buffers.append(self.tokens[current_position : current_position + (B * T + 1)])\n","            current_position += B * T\n","\n","        random.shuffle(buffers)\n","\n","        split_idx = int(len(buffers) * (1 - val_split))\n","        self.train = buffers[:split_idx]\n","        self.val = buffers[split_idx:]\n","\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}\")\n","\n","        self.current_train_buffer = 0\n","        self.current_val_buffer = 0\n","\n","    def next_batch(self, train=True):\n","        B, T = self.B, self.T\n","        if train:\n","            if self.current_train_buffer == 0:\n","                print(f\"Shuffling training buffers\")\n","                random.shuffle(self.train)\n","            buf = self.train[self.current_train_buffer]\n","            self.current_train_buffer = (self.current_train_buffer + 1) % len(self.train)\n","        else:\n","            if self.current_val_buffer == 0:\n","                print(f\"Shuffling validation buffers\")\n","                random.shuffle(self.val)\n","            buf = self.val[self.current_val_buffer]\n","            self.current_val_buffer = (self.current_val_buffer + 1) % len(self.val)\n","\n","\n","        # Ensure data is returned on the CPU, only transfer to GPU in training loop\n","        x = (buf[:-1]).view(B, T).to('cpu')  # inputs on CPU\n","        y = (buf[1:]).view(B, T).to('cpu')   # targets on CPU\n","        return x, y\n","\n","    def reset_buffers(self, train=True):\n","        if train:\n","            self.current_train_buffer = 0\n","        else:\n","            self.current_val_buffer = 0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22029,"status":"ok","timestamp":1724616164119,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"2AEWtIA3Bili","outputId":"2fe5babd-77eb-4ef0-f279-39ac4e2bc2fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]},{"output_type":"execute_result","data":{"text/plain":["OptimizedModule(\n","  (_orig_mod): GPT(\n","    (transformer): ModuleDict(\n","      (wte): Embedding(50304, 1024)\n","      (wpe): Embedding(1024, 1024)\n","      (h): ModuleList(\n","        (0-23): 24 x Block(\n","          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attn): CausalSelfAttention(\n","            (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n","            (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n","            (gelu): GELU(approximate='tanh')\n","            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":2}],"source":["# attempt to autodetect the device\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","    device = \"mps\"\n","print(f\"using device: {device}\")\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","model = GPT(GPTConfig(vocab_size=50304))\n","\n","model.to(device)\n","model = torch.compile(model)\n","model.load_state_dict(torch.load('model_medium_fifth_500.pth'), strict=False)\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"id":"BtvuKGFb6rIH","executionInfo":{"status":"error","timestamp":1724616193791,"user_tz":-60,"elapsed":29682,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"a9377da5-df4d-4e12-e675-cf7660efbeaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 98, with 354549760 parameters\n","num non-decayed parameter tensors: 194, with 321536 parameters\n","using fused AdamW: True\n","total desired batch size: 524288\n","=> calculated gradient accumulation steps: 32\n","loaded 66732716 characters\n","loaded: 17125777 tokens\n","Training buffers: 836, Validation buffers: 209\n","len(train_loader.train) / grad_accum_steps=26.125\n","batches_in_epoch=26\n","max_epochs=2\n","max_steps=52\n","warmup_steps=2\n","Shuffling training buffers\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-965970b109f7>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcatch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0mcatch_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchdynamo_orig_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             result = inner_convert(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mfail_user_frame_lineno\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mguarded_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         except (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mcleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_compile_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcleanup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mspeculation_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_pointer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                 ):\n\u001b[1;32m    812\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             )\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"RETURN_VALUE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mRETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         )\n\u001b[1;32m   2267\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RETURN_VALUE triggered compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m         self.output.compile_subgraph(\n\u001b[0m\u001b[1;32m   2269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m             reason=GraphCompileReason(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 output.extend(\n\u001b[0;32m-> 1001\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_output_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m                 )\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_global_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_correctness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mcompiler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compiler_fn did not return callable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_gm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_fx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_compiler_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0mtracing_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     ), compiled_autograd.disable():\n\u001b[0;32m-> 1330\u001b[0;31m         return aot_autograd(\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mfw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0mbw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\u001b[0m in \u001b[0;36mcompiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# NB: NOT cloned!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0menable_aot_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_module_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aot_autograd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcompiled_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         compiled_fn = create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} (dynamo_timed)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcompilation_time_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_spent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;31m# nonsensical. This does not affect the collection of metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch.cuda.set_rng_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 fw_metadata = run_functionalized_fw_and_collect_metadata(\n\u001b[0m\u001b[1;32m    534\u001b[0m                     \u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                     \u001b[0mkeep_input_mutations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maot_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_inference_input_mutations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/collect_metadata_analysis.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mflat_f_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mflat_f_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_f_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprior_autocast_states\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_get_autocast_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\u001b[0m in \u001b[0;36mfunctional_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m                     )\n\u001b[1;32m    675\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                         out = PropagateUnbackedSymInts(mod).run(\n\u001b[0m\u001b[1;32m    677\u001b[0m                             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_traceback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;31m# TODO: handle Tensor returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"example_value\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Main Node running APIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Execute the function and return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_subclasses/functional_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freeze_functional_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts_unwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                     outs_wrapped = pytree.tree_map_only(\n\u001b[0m\u001b[1;32m    420\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts_unwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map_only\u001b[0;34m(__type_or_types_or_pred, func, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPyTree\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m ) -> PyTree:\n\u001b[0;32m-> 1072\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__type_or_types_or_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36munflatten\u001b[0;34m(self, leaves)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_leaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_subclasses/functional_tensor.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionalTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_functional_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFunctionalTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_subclasses/functional_tensor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, elem)\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         out = torch.Tensor._make_wrapper_subclass(  # type: ignore[arg-type, attr-defined]\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;31m# TODO: right now, _make_wrapper_subclass's dynamic shape interaction is not great.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Calling the overload that has kwargs causes us to go down the first overload path,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","# optimize!\n","optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n","\n","total_batch_size = 524288  # 2**19, ~0.5M, in number of tokens\n","B = 16  # micro batch size\n","T = 1024  # sequence length\n","assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n","grad_accum_steps = total_batch_size // (B * T)\n","\n","print(f\"total desired batch size: {total_batch_size}\")\n","print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n","\n","torch.set_float32_matmul_precision('high')\n","\n","train_loader = DataLoaderLite(B, T, val_split=0.2)\n","\n","print(f\"{len(train_loader.train) / grad_accum_steps=}\")\n","batches_in_epoch = len(train_loader.train) // grad_accum_steps\n","print(f\"{batches_in_epoch=}\")\n","max_epochs = 100\n","max_steps = batches_in_epoch * max_epochs\n","print(f\"{max_epochs=}\")\n","print(f\"{max_steps=}\")\n","\n","max_lr = 6e-5\n","min_lr = max_lr * 0.1\n","warmup_steps = math.floor(0.05 * max_steps)\n","check_point = 10\n","interval = 2\n","print(f\"{warmup_steps=}\")\n","\n","def get_lr(it):\n","    # 1) Linear warmup for warmup_iter steps\n","    if it < warmup_steps:\n","        return max_lr * (it + 1) / warmup_steps\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > max_steps:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n","    return min_lr + coeff * (max_lr - min_lr)\n","\n","#raise SystemExit\n","\n","train_losses = []\n","val_losses = []\n","total_steps = 0\n","for epoch in range(max_epochs):\n","    # Training\n","    model.train()\n","    running_train_loss = 0\n","    for step in range(batches_in_epoch):\n","        t0 = time.time()\n","        optimizer.zero_grad()\n","        loss_accum = 0\n","\n","        for micro_step in range(grad_accum_steps):\n","            x, y = train_loader.next_batch(train=True)\n","            x, y = x.to(device), y.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits, loss = model(x, y)\n","            loss = loss / grad_accum_steps\n","            loss_accum += loss.detach()\n","            loss.backward()\n","\n","        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        lr = get_lr(step + epoch * batches_in_epoch)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","        optimizer.step()\n","\n","        running_train_loss += loss_accum.item()\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","        dt = t1 - t0  # time difference in seconds\n","        tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n","        tokens_per_sec = tokens_processed / dt\n","        total_steps += 1\n","        if step % interval == 0:\n","            print(f\"epoch {epoch} | step {total_steps} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tokens/sec: {tokens_per_sec:.2f}\")\n","\n","    avg_train_loss = running_train_loss / batches_in_epoch\n","    train_losses.append(avg_train_loss)\n","\n","    # Validation\n","    model.eval()\n","    running_val_loss = 0\n","    with torch.no_grad():\n","        for _ in range(len(train_loader.val)):\n","            x_val, y_val = train_loader.next_batch(train=False)\n","            x_val, y_val = x_val.to(device), y_val.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits_val, loss_val = model(x_val, y_val)\n","            running_val_loss += loss_val.item()\n","\n","    avg_val_loss = running_val_loss / len(train_loader.val)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f\"End of epoch {epoch} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Optionally, save the model checkpoint here\n","    if epoch % check_point == 0:\n","        print(\"Saving checkpoint\")\n","        torch.save(model.state_dict(), f\"model_medium_test_{epoch}.pth\")\n","    train_loader.reset_buffers(train=True)\n","    train_loader.reset_buffers(train=False)\n","\n","torch.save(model.state_dict(), f\"model_medium_test_final.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYznXzZBU3eD"},"outputs":[],"source":["import torch\n","\n","def generate_text(model, start_text, max_length=50, temperature=1.0, top_k=None):\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Tokenize the starting text\n","    enc = tiktoken.get_encoding('gpt2')\n","    input_ids = enc.encode(start_text, allowed_special={'<|endoftext|>'})\n","    input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n","\n","    generated_text = start_text\n","\n","    with torch.no_grad():\n","        for _ in range(max_length):\n","            outputs = model(input_ids)\n","            logits = outputs[0][:, -1, :]  # Get logits of the last token\n","\n","            # Apply temperature and top-k sampling\n","            logits = logits / temperature\n","            if top_k is not None:\n","                logits = top_k_logits(logits, top_k)\n","\n","            probs = torch.nn.functional.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            next_token_id = next_token.item()\n","\n","            # Add the predicted token to the sequence\n","            input_ids = torch.cat((input_ids, next_token), dim=1)\n","            generated_text += enc.decode([next_token_id])\n","\n","            # Stop if end-of-text token is generated\n","            if next_token_id == enc.eot_token:\n","                break\n","\n","    return generated_text\n","\n","def top_k_logits(logits, k):\n","    \"\"\" Set all logits but the k highest to -infinity \"\"\"\n","    values, indices = torch.topk(logits, k)\n","    out = logits.clone()\n","    out[out < values[..., -1, None]] = -float('Inf')\n","    return out\n","\n","\n"]},{"cell_type":"code","source":["# Example usage:\n","start_text = \"In the dark room the man stood\"\n","generated = generate_text(model, start_text, max_length=1000, temperature=1.0, top_k=50)\n","print(generated)"],"metadata":{"id":"T1wU8mgNKYte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"_237cAixOsHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import ast\n","import unicodedata\n","\n","def normalize_text(text):\n","    # Replace curly quotes with straight quotes\n","    text = re.sub(r'[‘’]', '\\'', text)  # Replace left and right single quotes with '\n","    text = re.sub(r'[“”]', '\"', text)   # Replace left and right double quotes with \"\n","\n","    # Replace en dash and em dash with a hyphen or double hyphen\n","    text = re.sub(r'–', '-', text)  # Replace en dash with hyphen\n","    text = re.sub(r'—', '--', text) # Replace em dash with double hyphen\n","\n","    # Replace ellipsis with three dots\n","    text = re.sub(r'…', '...', text)\n","\n","    # Replace other non-ASCII punctuation with ASCII equivalents\n","    text = re.sub(r'«', '<<', text)  # Replace left double angle quote with <<\n","    text = re.sub(r'»', '>>', text)  # Replace right double angle quote with >>\n","    text = re.sub(r'§', 'SS', text)  # Replace section sign with SS\n","    text = re.sub(r'•', '*', text)   # Replace bullet with asterisk\n","    text = re.sub(r'‽', '?!', text)  # Replace interrobang with ?!\n","    text = re.sub(r'′', \"'\", text)   # Replace prime with single quote\n","    text = re.sub(r'″', '\"', text)   # Replace double prime with double quote\n","    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n","\n","def get_conversations_HF(data_path):\n","    conversations = []\n","\n","    # Load movie lines\n","    id2line = {}\n","    with open(os.path.join(data_path, 'movie_lines.txt'), 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            parts = line.strip().split(' +++$+++ ')\n","            if len(parts) == 5:\n","                line_id, text = parts[0], parts[4]\n","                id2line[line_id] = text\n","\n","    # Load conversations\n","    with open(os.path.join(data_path, 'movie_conversations.txt'), 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            parts = line.strip().split(' +++$+++ ')\n","            if len(parts) == 4:\n","                conv_line_ids = ast.literal_eval(parts[3])  # safer than eval()\n","                # Create pairs of conversations (input, response)\n","                for i in range(len(conv_line_ids) - 1):\n","                    # Ensure both line IDs are in id2line\n","                    if conv_line_ids[i] in id2line and conv_line_ids[i + 1] in id2line:\n","                        input_line = \"Q: \" + normalize_text(id2line[conv_line_ids[i]])\n","                        response_line = \"A: \" + normalize_text(id2line[conv_line_ids[i + 1]])\n","                        conversations.append((input_line, response_line))\n","                    #else:\n","                    #    print(f\"Missing line ID in conversation: {conv_line_ids[i]} or {conv_line_ids[i + 1]}\")\n","\n","    return conversations\n","\n","\n","class DataLoaderChat:\n","    def __init__(self, B, T, val_split=0.2):\n","        self.B = B\n","        self.T = T\n","\n","        data_path = \"data/cornell_movie_dialogs_corpus\"\n","        conversations = get_conversations_HF(data_path)\n","\n","        con = [ q + '\\n' + a + '<|endoftext|>' for q, a in conversations ]\n","\n","        text = ''.join(con)\n","        print(f\"{len(text)=}\")\n","        enc = tiktoken.get_encoding('gpt2')\n","\n","        tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n","        len_tokens = len(tokens)\n","        print(f\"{len(tokens)=}\")\n","        # Ensure tokens are stored on CPU\n","        self.tokens = torch.tensor(tokens, device='cpu')\n","        print(f\"loaded: {len(self.tokens)} tokens\")\n","\n","        # Split into training and validation buffers on CPU\n","        buffers = []\n","        current_position = 0\n","        while current_position + (B * T + 1) < len_tokens:\n","            buffers.append(self.tokens[current_position : current_position + (B * T + 1)])\n","            current_position += B * T\n","\n","        random.shuffle(buffers)\n","\n","        split_idx = int(len(buffers) * (1 - val_split))\n","        self.train = buffers[:split_idx]\n","        self.val = buffers[split_idx:]\n","\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}\")\n","\n","        self.current_train_buffer = 0\n","        self.current_val_buffer = 0\n","\n","    def next_batch(self, train=True):\n","        B, T = self.B, self.T\n","        if train:\n","            if self.current_train_buffer == 0:\n","                print(f\"Shuffling training buffers\")\n","                random.shuffle(self.train)\n","            buf = self.train[self.current_train_buffer]\n","            self.current_train_buffer = (self.current_train_buffer + 1) % len(self.train)\n","        else:\n","            if self.current_val_buffer == 0:\n","                print(f\"Shuffling validation buffers\")\n","                random.shuffle(self.val)\n","            buf = self.val[self.current_val_buffer]\n","            self.current_val_buffer = (self.current_val_buffer + 1) % len(self.val)\n","\n","\n","        # Ensure data is returned on the CPU, only transfer to GPU in training loop\n","        x = (buf[:-1]).view(B, T).to('cpu')  # inputs on CPU\n","        y = (buf[1:]).view(B, T).to('cpu')   # targets on CPU\n","        return x, y\n","\n","    def reset_buffers(self, train=True):\n","        if train:\n","            self.current_train_buffer = 0\n","        else:\n","            self.current_val_buffer = 0"],"metadata":{"id":"PepcnJHz429s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","# optimize!\n","optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n","\n","B = 16  # micro batch size\n","T = 1024  # sequence length\n","total_batch_size = 4 * B * T\n","assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n","grad_accum_steps = total_batch_size // (B * T)\n","\n","print(f\"total desired batch size: {total_batch_size}\")\n","print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n","\n","torch.set_float32_matmul_precision('high')\n","\n","train_loader = DataLoaderChat(B, T, val_split=0.2)\n","\n","print(f\"{len(train_loader.train) / grad_accum_steps=}\")\n","batches_in_epoch = len(train_loader.train) // grad_accum_steps\n","print(f\"{batches_in_epoch=}\")\n","max_epochs = 10\n","max_steps = batches_in_epoch * max_epochs\n","print(f\"{max_epochs=}\")\n","print(f\"{max_steps=}\")\n","\n","max_lr = 6e-5\n","min_lr = max_lr * 0.1\n","warmup_steps = math.floor(0.05 * max_steps)\n","check_point = 10\n","interval = 2\n","print(f\"{warmup_steps=}\")\n","\n","def get_lr(it):\n","    # 1) Linear warmup for warmup_iter steps\n","    if it < warmup_steps:\n","        return max_lr * (it + 1) / warmup_steps\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > max_steps:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n","    return min_lr + coeff * (max_lr - min_lr)\n","\n","#raise SystemExit\n","\n","train_losses = []\n","val_losses = []\n","total_steps = 0\n","for epoch in range(max_epochs):\n","    # Training\n","    model.train()\n","    running_train_loss = 0\n","    for step in range(batches_in_epoch):\n","        t0 = time.time()\n","        optimizer.zero_grad()\n","        loss_accum = 0\n","\n","        for micro_step in range(grad_accum_steps):\n","            x, y = train_loader.next_batch(train=True)\n","            x, y = x.to(device), y.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits, loss = model(x, y)\n","            loss = loss / grad_accum_steps\n","            loss_accum += loss.detach()\n","            loss.backward()\n","\n","        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        lr = get_lr(step + epoch * batches_in_epoch)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","        optimizer.step()\n","\n","        running_train_loss += loss_accum.item()\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","        dt = t1 - t0  # time difference in seconds\n","        tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n","        tokens_per_sec = tokens_processed / dt\n","        total_steps += 1\n","        if step % interval == 0:\n","            print(f\"epoch {epoch} | step {total_steps} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tokens/sec: {tokens_per_sec:.2f}\")\n","\n","    avg_train_loss = running_train_loss / batches_in_epoch\n","    train_losses.append(avg_train_loss)\n","\n","    # Validation\n","    model.eval()\n","    running_val_loss = 0\n","    with torch.no_grad():\n","        for _ in range(len(train_loader.val)):\n","            x_val, y_val = train_loader.next_batch(train=False)\n","            x_val, y_val = x_val.to(device), y_val.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits_val, loss_val = model(x_val, y_val)\n","            running_val_loss += loss_val.item()\n","\n","    avg_val_loss = running_val_loss / len(train_loader.val)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f\"End of epoch {epoch} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Optionally, save the model checkpoint here\n","    if epoch % check_point == 0:\n","        print(\"Saving checkpoint\")\n","        torch.save(model.state_dict(), f\"model_medium_chat_{epoch}.pth\")\n","    train_loader.reset_buffers(train=True)\n","    train_loader.reset_buffers(train=False)\n","\n","torch.save(model.state_dict(), f\"model_medium_chat_final.pth\")"],"metadata":{"id":"IaO_6J3L_EQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"DN_896GZDrsD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage:\n","start_text = \"Q: Who and what are you?\\nA: \"\n","generated = generate_text(model, start_text, max_length=1000, temperature=1.0, top_k=50)\n","print(generated)"],"metadata":{"id":"sUX8dUbp_EEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","\n","\n","\n","def test():\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Tokenize the starting text\n","    tokenizer = tiktoken.get_encoding('gpt2')\n","\n","    generated = ''\n","\n","    while True:\n","        try:\n","            input_text = input(\"> \")\n","            current_length = len(generated)\n","            generated += \"Q: \" + input_text + \"\\nA:\"\n","\n","\n","            generated = generate_text(model, generated, max_length=100, temperature=1.0, top_k=50)\n","\n","            print(generated[current_length :].replace('<|endoftext|>','\\n'))\n","\n","        except Exception as e:\n","            print(e)\n","\n","test()"],"metadata":{"id":"yxg1hz_dBNv_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CONVERSATION TRAINING**"],"metadata":{"id":"jYQfzoMqEfK5"}},{"cell_type":"code","source":["import math\n","import random\n","!pip install tiktoken\n","import tiktoken\n","import inspect\n","from dataclasses import dataclass\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# ---------------------------------------------\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","        # output projection\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","        # regularization\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","            .view(1, 1, config.block_size, config.block_size))\n","\n","    def forward(self, x):\n","        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n","        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n","        qkv = self.c_attn(x)\n","        q, k, v = qkv.split(self.n_embd, dim=2)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        # attention (materializes the large (T*T) matrix for all the queries and keys)\n","        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n","        # att = F.softmax(att, dim=-1)\n","        # y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n","        # output projection\n","        y = self.c_proj(y)\n","        return y\n","\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n","        self.gelu = nn.GELU(approximate='tanh')\n","        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n","        self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(config.n_embd)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = nn.LayerNorm(config.n_embd)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 1024  # max sequence length\n","    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    # 'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","    # 'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","    # 'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","    # 'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","    n_layer: int = 24  # number of layers\n","    n_head: int = 16  # number of heads\n","    n_embd: int = 1024  # embedding dimension\n","\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = nn.LayerNorm(config.n_embd),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        # weight sharing scheme\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        # init params\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            std = 0.02\n","            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","                std *= (2 * self.config.n_layer) ** -0.5\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","\n","\n","\n","    def forward(self, idx, targets=None):\n","        # idx is of shape (B, T)\n","        B, T = idx.size()\n","        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n","        # forward the token and position embeddings\n","        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n","        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n","        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n","        x = tok_emb + pos_emb\n","        # forward the blocks of the transformer\n","        for block in self.transformer.h:\n","            x = block(x)\n","        # forward the final layernorm and the classifier\n","        x = self.transformer.ln_f(x)\n","        logits = self.lm_head(x)  # (B, T, vocab_size)\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","        return logits, loss\n","\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type):\n","        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        from transformers import GPT2LMHeadModel\n","        print(\"loading weights from pretrained gpt: %s\" % model_type)\n","\n","        # n_layer, n_head and n_embd are determined from model_type\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n","        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n","        # create a from-scratch initialized minGPT model\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n","\n","        # init a huggingface/transformers model\n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = model_hf.state_dict()\n","\n","        # copy while ensuring all of the parameters are aligned and match in names and shapes\n","        sd_keys_hf = sd_hf.keys()\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n","        # this means that we have to transpose these weights when we import them\n","        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                # special treatment for the Conv1D weights we need to transpose\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                # vanilla copy over the other parameters\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device):\n","        # start with all of the candidate parameters (that require grad)\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n","        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': nodecay_params, 'weight_decay': 0.0}\n","        ]\n","\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n","        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params} parameters\")\n","        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\")\n","\n","        # Create AdamW optimizer and use the fused version if it is available\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and 'cuda' in device\n","        print(f\"using fused AdamW: {use_fused}\")\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n","        return optimizer\n","\n","\n","# attempt to autodetect the device\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","    device = \"mps\"\n","print(f\"using device: {device}\")\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","model = GPT(GPTConfig(vocab_size=50304))\n","\n","model.to(device)\n","model = torch.compile(model)\n","model.load_state_dict(torch.load('model_medium_fifth_500.pth'), strict=False)\n","model.train()"],"metadata":{"id":"YbD5hjMYInp1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724639295564,"user_tz":-60,"elapsed":16016,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"a1d37ef0-393e-4c6f-fb60-f667ba6ce028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n","using device: cuda\n"]},{"output_type":"execute_result","data":{"text/plain":["OptimizedModule(\n","  (_orig_mod): GPT(\n","    (transformer): ModuleDict(\n","      (wte): Embedding(50304, 1024)\n","      (wpe): Embedding(1024, 1024)\n","      (h): ModuleList(\n","        (0-23): 24 x Block(\n","          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attn): CausalSelfAttention(\n","            (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n","            (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n","            (gelu): GELU(approximate='tanh')\n","            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["!pip install datasets\n","from datasets import load_dataset\n","\n","# Load the DailyDialog dataset\n","dataset = load_dataset(\"daily_dialog\")\n","\n","# Check the available splits\n","print(dataset)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygbOdKFAEcgt","executionInfo":{"status":"ok","timestamp":1724639311488,"user_tz":-60,"elapsed":12049,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"ff0b9f7c-38df-45ae-96de-a788b20291d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 11118\n","    })\n","    validation: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","})\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import ast\n","import json\n","from cleaner import normalize_text\n","\n","class DataLoaderChat:\n","    def __init__(self, B, T):\n","\n","        self.train = None\n","        self.val = None\n","        self.test = None\n","\n","        self.B = B\n","        self.T = T\n","\n","        # First get Hugging Face data\n","        print(f\"loading Hugging Face data\")\n","        train, val, test = self.get_conversations_HF()\n","        self.process_conversations(train, val, test)\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}, Test buffers: {len(self.test)}\")\n","\n","        # Now get Cornell data\n","        print(f\"loading Cornell data\")\n","        data_path = \"data/cornell_movie_dialogs_corpus\"\n","        train, val, test = self.get_conversations_cornell(data_path)\n","        self.process_conversations(train, val, test)\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}, Test buffers: {len(self.test)}\")\n","\n","        # Now get woz data\n","        print(f\"loading woz data\")\n","        data_path = 'data/MultiWOZ_2.2'\n","        train, val, test = self.get_conversations_woz(data_path)\n","        self.process_conversations(train, val, test)\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}, Test buffers: {len(self.test)}\")\n","\n","        # Now get taskmaster data\n","        print(f\"loading taskmaster data\")\n","        data_path = 'data/Taskmaster'\n","        train, val, test = self.get_conversations_taskmaster(data_path)\n","        self.process_conversations(train, val, test)\n","        print(f\"Training buffers: {len(self.train)}, Validation buffers: {len(self.val)}, Test buffers: {len(self.test)}\")\n","\n","        random.shuffle(self.train)\n","        random.shuffle(self.val)\n","        if self.test:\n","            random.shuffle(self.test)\n","\n","        self.current_train_buffer = 0\n","        self.current_val_buffer = 0\n","        self.current_test_buffer = 0\n","\n","    def get_conversations_HF(self):\n","        train = []\n","        val = []\n","        test = []\n","        the_datasets = {'train': train, 'validation': val, 'test': test}\n","\n","        types = ['train', 'validation', 'test']\n","\n","        pattern = r'\\s([,.!?;:])'\n","        # Regular expressions for spaces around parentheses\n","        pattern_left_parenthesis = r'\\(\\s'\n","        pattern_right_parenthesis = r'\\s\\)'\n","\n","        for t in types:\n","            print(f\"loading: {t=}\")\n","            ds = dataset[t]\n","            # Access specific fields\n","            print(f\"{len(ds)=}\")\n","\n","            for i in range(len(ds)):\n","                dialogue = ds[i]['dialog']\n","                turn = [\"Q: \", \"A: \"]\n","                conv = []\n","                for i, s in enumerate(dialogue):\n","                    s = s.strip()\n","                    s = normalize_text(s)\n","                    s = s.replace(\" ' \", \"'\")\n","                    s = s.replace('$ ', '$')\n","                    s = s.replace('( ', '(')\n","                    s = s.replace(') ', ')')\n","                    s = re.sub(pattern, r'\\1', s)\n","                    s = turn[0 if (i+1)%2 == 1 else 1] + s\n","                    conv.append(s)\n","                the_datasets[t].append(tuple(conv))\n","\n","        return the_datasets['train'], the_datasets['validation'], the_datasets['test']\n","\n","    def get_conversations_cornell(self, data_path, val_split=0.2):\n","        conversations = []\n","\n","        # Load movie lines\n","        id2line = {}\n","        with open(os.path.join(data_path, 'movie_lines.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 5:\n","                    line_id, text = parts[0], parts[4]\n","                    id2line[line_id] = text\n","\n","        # Load conversations\n","        with open(os.path.join(data_path, 'movie_conversations.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 4:\n","                    conv_line_ids = ast.literal_eval(parts[3])  # safer than eval()\n","                    # Create pairs of conversations (input, response)\n","                    for i in range(len(conv_line_ids) - 1):\n","                        # Ensure both line IDs are in id2line\n","                        if conv_line_ids[i] in id2line and conv_line_ids[i + 1] in id2line:\n","                            input_line = \"Q: \" + normalize_text(id2line[conv_line_ids[i]])\n","                            response_line = \"A: \" + normalize_text(id2line[conv_line_ids[i + 1]])\n","                            conversations.append((input_line, response_line))\n","                        #else:\n","                        #    print(f\"Missing line ID in conversation: {conv_line_ids[i]} or {conv_line_ids[i + 1]}\")\n","\n","        print(f\"{len(conversations)=}\")\n","        random.shuffle(conversations)\n","        train = conversations[:-int(len(conversations) * val_split)]\n","        val = conversations[-int(len(conversations) * val_split):]\n","        test = None\n","\n","        return train, val, test\n","\n","    def get_conversations_woz(self, data_path):\n","        turn = ['Q: ', 'A: ']\n","        types = ['train', 'dev', 'test']\n","        the_datasets = {t : [] for t in types}\n","        dir = {t : os.path.join(data_path, t) for t in types}\n","        for t in types:\n","            print(f\"processing: {t=}\")\n","            json_files = [f for f in os.listdir(dir[t]) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            for json_file in json_files:\n","                file_path = os.path.join(dir[t], json_file)\n","                with open(file_path, 'r') as file:\n","                    dialogues.extend(json.load(file))\n","            for dialogue in dialogues:\n","                conversation_list = []\n","                conversation = dialogue['turns']\n","                for i, line in enumerate(conversation):\n","                    conversation_list.append(turn[0 if (i+1)%2 == 1 else 1] + normalize_text(line['utterance']))\n","                the_datasets[t].append(tuple(conversation_list))\n","            print(f\"{len(the_datasets[t])=}\")\n","\n","        return the_datasets['train'], the_datasets['dev'], the_datasets['test']\n","\n","    def get_conversations_taskmaster(self, data_path, val_split = 0.2):\n","        dirs = ['TM-1-2019', 'TM-2-2020/data', 'TM-3-2020/data', 'TM-4-2024/data']\n","        conversations = []\n","        for dir in dirs:\n","            print(f\"{dir=}\")\n","            path = os.path.join(data_path, dir)\n","            json_files = [f for f in os.listdir(path) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            print(\"loading JSON files\")\n","            for json_file in json_files:\n","                file_path = os.path.join(path, json_file)\n","                with open(file_path, 'r') as file:\n","                    print(f\"{file_path=}\")\n","                    dialogues.extend(json.load(file))\n","            print(\"processing dialogues\")\n","            for dialogue in dialogues:\n","                utterances = dialogue['utterances']\n","                conversation_list = []\n","                previous_prompt = \"\"\n","                for line in utterances:\n","                    prompt = \"A: \"\n","                    if line['speaker'].lower() == 'user':\n","                        prompt = \"Q: \"\n","                    if previous_prompt == prompt:\n","                        last_text = conversation_list.pop()\n","                        conversation_list.append(last_text + \" \" + normalize_text(line['text']))\n","                    else:\n","                        conversation_list.append(prompt + normalize_text(line['text']))\n","                        previous_prompt = prompt\n","                conversations.append(tuple(conversation_list))\n","        print(f\"{len(conversations)=}\")\n","        random.shuffle(conversations)\n","        train = conversations[:-int(len(conversations) * val_split)]\n","        val = conversations[-int(len(conversations) * val_split):]\n","        test = None\n","        return train, val, test\n","\n","    def process_conversations(self, train, val, test):\n","        types = ['train', 'validation', 'test']\n","        the_conversations = {'train': train, 'validation': val, 'test': test}\n","\n","        for t in types:\n","            print(f\"processing: {t=}\")\n","            conversations = the_conversations[t]\n","            if not conversations:\n","                print(f\"No conversations found for type: {t}\")\n","                continue\n","            conversations_list = []\n","            dialogue_list = []\n","            for dialogue in  conversations:\n","                for line in dialogue:\n","                    dialogue_list.append(line + '\\n')\n","                dialogue_list.append('<|endoftext|>')\n","                conversations_list.append(''.join(dialogue_list))\n","                dialogue_list = []\n","\n","            text = ''.join(conversations_list)\n","            print(f\"{len(text)=}\")\n","\n","            enc = tiktoken.get_encoding('gpt2')\n","            tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n","            len_tokens = len(tokens)\n","            print(f\"{len(tokens)=}\")\n","            # Ensure tokens are stored on CPU\n","            tokens = torch.tensor(tokens, device='cpu')\n","            print(f\"loaded: {len(tokens)} tokens\")\n","\n","            # Split into training and validation buffers on CPU\n","            buffers = []\n","            current_position = 0\n","            while current_position + (B * T + 1) < len_tokens:\n","                buffers.append(tokens[current_position : current_position + (B * T + 1)])\n","                current_position += B * T\n","\n","            if t == 'train':\n","                if self.train:\n","                    self.train.extend(buffers)\n","                else:\n","                    self.train = buffers\n","            elif t == 'validation':\n","                if self.val:\n","                    self.val.extend(buffers)\n","                else:\n","                    self.val = buffers\n","            else:\n","                if self.test:\n","                    self.test.extend(buffers)\n","                else:\n","                    self.test = buffers\n","\n","    def next_batch(self, train=True):\n","        B, T = self.B, self.T\n","        if train:\n","            if self.current_train_buffer == 0:\n","                print(f\"Shuffling training buffers\")\n","                random.shuffle(self.train)\n","            buf = self.train[self.current_train_buffer]\n","            self.current_train_buffer = (self.current_train_buffer + 1) % len(self.train)\n","        else:\n","            if self.current_val_buffer == 0:\n","                print(f\"Shuffling validation buffers\")\n","                random.shuffle(self.val)\n","            buf = self.val[self.current_val_buffer]\n","            self.current_val_buffer = (self.current_val_buffer + 1) % len(self.val)\n","\n","\n","        # Ensure data is returned on the CPU, only transfer to GPU in training loop\n","        x = (buf[:-1]).view(B, T).to('cpu')  # inputs on CPU\n","        y = (buf[1:]).view(B, T).to('cpu')   # targets on CPU\n","        return x, y\n","\n","    def reset_buffers(self, train=True):\n","        if train:\n","            self.current_train_buffer = 0\n","        else:\n","            self.current_val_buffer = 0\n","\n","\n"],"metadata":{"id":"pSc2YsY6E7-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","# optimize!\n","optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n","\n","B = 16  # micro batch size\n","T = 1024  # sequence length\n","total_batch_size = 16 * B * T\n","assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n","grad_accum_steps = total_batch_size // (B * T)\n","\n","print(f\"total desired batch size: {total_batch_size}\")\n","print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n","\n","torch.set_float32_matmul_precision('high')\n","\n","train_loader = DataLoaderChat(B, T)\n","\n","print(f\"{len(train_loader.train) / grad_accum_steps=}\")\n","batches_in_epoch = len(train_loader.train) // grad_accum_steps\n","print(f\"{batches_in_epoch=}\")\n","max_epochs = 10\n","max_steps = batches_in_epoch * max_epochs\n","print(f\"{max_epochs=}\")\n","print(f\"{max_steps=}\")\n","\n","max_lr = 6e-5\n","min_lr = max_lr * 0.1\n","warmup_steps = math.floor(0.05 * max_steps)\n","check_point = 50\n","interval = 10\n","print(f\"{warmup_steps=}\")\n","\n","def get_lr(it):\n","    # 1) Linear warmup for warmup_iter steps\n","    if it < warmup_steps:\n","        return max_lr * (it + 1) / warmup_steps\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > max_steps:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n","    return min_lr + coeff * (max_lr - min_lr)\n","\n","train_losses = []\n","val_losses = []\n","total_steps = 0\n","for epoch in range(max_epochs):\n","    # Training\n","    model.train()\n","    running_train_loss = 0\n","    for step in range(batches_in_epoch):\n","        t0 = time.time()\n","        optimizer.zero_grad()\n","        loss_accum = 0\n","\n","        for micro_step in range(grad_accum_steps):\n","            x, y = train_loader.next_batch(train=True)\n","            x, y = x.to(device), y.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits, loss = model(x, y)\n","            loss = loss / grad_accum_steps\n","            loss_accum += loss.detach()\n","            loss.backward()\n","\n","        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        lr = get_lr(step + epoch * batches_in_epoch)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","        optimizer.step()\n","\n","        running_train_loss += loss_accum.item()\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","        dt = t1 - t0  # time difference in seconds\n","        tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n","        tokens_per_sec = tokens_processed / dt\n","        total_steps += 1\n","        if total_steps % interval == 0:\n","            print(f\"epoch {epoch} | total_steps {total_steps} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tokens/sec: {tokens_per_sec:.2f}\")\n","\n","    avg_train_loss = running_train_loss / batches_in_epoch\n","    train_losses.append(avg_train_loss)\n","\n","    # Validation\n","    model.eval()\n","    running_val_loss = 0\n","    with torch.no_grad():\n","        for _ in range(len(train_loader.val)):\n","            x_val, y_val = train_loader.next_batch(train=False)\n","            x_val, y_val = x_val.to(device), y_val.to(device)\n","            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n","                logits_val, loss_val = model(x_val, y_val)\n","            running_val_loss += loss_val.item()\n","\n","    avg_val_loss = running_val_loss / len(train_loader.val)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f\"End of epoch {epoch} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Optionally, save the model checkpoint here\n","    if epoch % check_point == 0:\n","        print(\"Saving checkpoint\")\n","        torch.save(model.state_dict(), f\"model_medium_chat_HF_{epoch}.pth\")\n","    train_loader.reset_buffers(train=True)\n","    train_loader.reset_buffers(train=False)\n","\n","torch.save(model.state_dict(), f\"model_medium_chat_HF_final.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoythYT7ToZ9","executionInfo":{"status":"ok","timestamp":1724666751381,"user_tz":-60,"elapsed":3865306,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"a526ef34-bc61-4be9-e5f7-4e21d86c6886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 98, with 354549760 parameters\n","num non-decayed parameter tensors: 194, with 321536 parameters\n","using fused AdamW: True\n","total desired batch size: 262144\n","=> calculated gradient accumulation steps: 16\n","loading Hugging Face data\n","loading: t='train'\n","len(ds)=11118\n","loading: t='validation'\n","len(ds)=1000\n","loading: t='test'\n","len(ds)=1000\n","processing: t='train'\n","len(text)=5511281\n","len(tokens)=1511279\n","loaded: 1511279 tokens\n","processing: t='validation'\n","len(text)=509503\n","len(tokens)=139417\n","loaded: 139417 tokens\n","processing: t='test'\n","len(text)=497244\n","len(tokens)=135733\n","loaded: 135733 tokens\n","Training buffers: 92, Validation buffers: 8, Test buffers: 8\n","loading Cornell data\n","len(conversations)=221282\n","processing: t='train'\n","len(text)=23041615\n","len(tokens)=6357644\n","loaded: 6357644 tokens\n","processing: t='validation'\n","len(text)=5785511\n","len(tokens)=1596736\n","loaded: 1596736 tokens\n","processing: t='test'\n","No conversations found for type: test\n","Training buffers: 480, Validation buffers: 105, Test buffers: 8\n","loading woz data\n","processing: t='train'\n","len(the_datasets[t])=8437\n","processing: t='dev'\n","len(the_datasets[t])=1000\n","processing: t='test'\n","len(the_datasets[t])=1000\n","processing: t='train'\n","len(text)=8422452\n","len(tokens)=2251972\n","loaded: 2251972 tokens\n","processing: t='validation'\n","len(text)=1114843\n","len(tokens)=298650\n","loaded: 298650 tokens\n","processing: t='test'\n","len(text)=1112547\n","len(tokens)=298241\n","loaded: 298241 tokens\n","Training buffers: 617, Validation buffers: 123, Test buffers: 26\n","loading taskmaster data\n","dir='TM-1-2019'\n","loading JSON files\n","file_path='data/Taskmaster/TM-1-2019/woz-dialogs.json'\n","file_path='data/Taskmaster/TM-1-2019/self-dialogs.json'\n","processing dialogues\n","dir='TM-2-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-2-2020/data/flights.json'\n","file_path='data/Taskmaster/TM-2-2020/data/music.json'\n","file_path='data/Taskmaster/TM-2-2020/data/hotels.json'\n","file_path='data/Taskmaster/TM-2-2020/data/restaurant-search.json'\n","file_path='data/Taskmaster/TM-2-2020/data/sports.json'\n","file_path='data/Taskmaster/TM-2-2020/data/movies.json'\n","file_path='data/Taskmaster/TM-2-2020/data/food-ordering.json'\n","processing dialogues\n","dir='TM-3-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-3-2020/data/data_15.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_05.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_18.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_09.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_16.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_02.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_14.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_07.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_00.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_04.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_03.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_01.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_11.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_12.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_10.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_19.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_13.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_17.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_08.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_06.json'\n","processing dialogues\n","dir='TM-4-2024/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-4-2024/data/data_04.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_01.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_06.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_00.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_02.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_03.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_05.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_07.json'\n","processing dialogues\n","len(conversations)=57986\n","processing: t='train'\n","len(text)=46773483\n","len(tokens)=13346609\n","loaded: 13346609 tokens\n","processing: t='validation'\n","len(text)=11649330\n","len(tokens)=3321819\n","loaded: 3321819 tokens\n","processing: t='test'\n","No conversations found for type: test\n","Training buffers: 1431, Validation buffers: 325, Test buffers: 26\n","len(train_loader.train) / grad_accum_steps=89.4375\n","batches_in_epoch=89\n","max_epochs=10\n","max_steps=890\n","warmup_steps=44\n","Shuffling training buffers\n","epoch 0 | total_steps 10 | loss: 2.089591 | lr 1.3636e-05 | norm: 2.8005 | dt: 3971.73ms | tokens/sec: 66002.50\n","epoch 0 | total_steps 20 | loss: 1.772473 | lr 2.7273e-05 | norm: 1.3311 | dt: 3990.69ms | tokens/sec: 65688.87\n","epoch 0 | total_steps 30 | loss: 1.525380 | lr 4.0909e-05 | norm: 1.2533 | dt: 3994.84ms | tokens/sec: 65620.64\n","epoch 0 | total_steps 40 | loss: 1.377754 | lr 5.4545e-05 | norm: 1.9880 | dt: 3999.22ms | tokens/sec: 65548.74\n","epoch 0 | total_steps 50 | loss: 1.668307 | lr 5.9995e-05 | norm: 1.4973 | dt: 4000.24ms | tokens/sec: 65532.06\n","epoch 0 | total_steps 60 | loss: 1.641286 | lr 5.9958e-05 | norm: 1.6752 | dt: 3999.68ms | tokens/sec: 65541.18\n","epoch 0 | total_steps 70 | loss: 1.533548 | lr 5.9884e-05 | norm: 1.7531 | dt: 4001.82ms | tokens/sec: 65506.17\n","epoch 0 | total_steps 80 | loss: 1.364514 | lr 5.9772e-05 | norm: 1.4928 | dt: 4003.72ms | tokens/sec: 65475.15\n","Shuffling validation buffers\n","End of epoch 0 | Avg Train Loss: 1.652393 | Avg Val Loss: 1.825520\n","Saving checkpoint\n","Shuffling training buffers\n","epoch 1 | total_steps 90 | loss: 1.477994 | lr 5.9624e-05 | norm: 1.2512 | dt: 4009.38ms | tokens/sec: 65382.71\n","epoch 1 | total_steps 100 | loss: 1.134852 | lr 5.9439e-05 | norm: 1.9584 | dt: 3992.96ms | tokens/sec: 65651.49\n","epoch 1 | total_steps 110 | loss: 1.763721 | lr 5.9217e-05 | norm: 1.6564 | dt: 3995.59ms | tokens/sec: 65608.33\n","epoch 1 | total_steps 120 | loss: 1.107891 | lr 5.8960e-05 | norm: 1.6904 | dt: 4000.49ms | tokens/sec: 65527.92\n","epoch 1 | total_steps 130 | loss: 1.481635 | lr 5.8666e-05 | norm: 1.2901 | dt: 4001.70ms | tokens/sec: 65508.24\n","epoch 1 | total_steps 140 | loss: 1.365649 | lr 5.8337e-05 | norm: 1.5048 | dt: 4003.66ms | tokens/sec: 65476.03\n","epoch 1 | total_steps 150 | loss: 1.372105 | lr 5.7973e-05 | norm: 2.0415 | dt: 4006.78ms | tokens/sec: 65425.11\n","epoch 1 | total_steps 160 | loss: 1.273041 | lr 5.7575e-05 | norm: 1.4734 | dt: 4002.67ms | tokens/sec: 65492.25\n","epoch 1 | total_steps 170 | loss: 1.171257 | lr 5.7143e-05 | norm: 1.5129 | dt: 4002.72ms | tokens/sec: 65491.53\n","Shuffling validation buffers\n","End of epoch 1 | Avg Train Loss: 1.365536 | Avg Val Loss: 1.831108\n","Shuffling training buffers\n","epoch 2 | total_steps 180 | loss: 1.147591 | lr 5.6678e-05 | norm: 1.9366 | dt: 4003.61ms | tokens/sec: 65476.94\n","epoch 2 | total_steps 190 | loss: 1.039732 | lr 5.6180e-05 | norm: 1.7854 | dt: 4004.94ms | tokens/sec: 65455.12\n","epoch 2 | total_steps 200 | loss: 1.222495 | lr 5.5650e-05 | norm: 1.5106 | dt: 4003.30ms | tokens/sec: 65481.95\n","epoch 2 | total_steps 210 | loss: 1.268172 | lr 5.5088e-05 | norm: 1.6497 | dt: 3999.86ms | tokens/sec: 65538.24\n","epoch 2 | total_steps 220 | loss: 1.398357 | lr 5.4497e-05 | norm: 1.2779 | dt: 4003.61ms | tokens/sec: 65476.93\n","epoch 2 | total_steps 230 | loss: 1.300873 | lr 5.3875e-05 | norm: 1.7035 | dt: 4002.47ms | tokens/sec: 65495.61\n","epoch 2 | total_steps 240 | loss: 1.395103 | lr 5.3225e-05 | norm: 1.6344 | dt: 3998.06ms | tokens/sec: 65567.83\n","epoch 2 | total_steps 250 | loss: 1.384088 | lr 5.2547e-05 | norm: 1.8773 | dt: 4003.58ms | tokens/sec: 65477.44\n","epoch 2 | total_steps 260 | loss: 1.185603 | lr 5.1842e-05 | norm: 1.9661 | dt: 4002.80ms | tokens/sec: 65490.16\n","Shuffling validation buffers\n","End of epoch 2 | Avg Train Loss: 1.228779 | Avg Val Loss: 1.865860\n","Shuffling training buffers\n","epoch 3 | total_steps 270 | loss: 1.228198 | lr 5.1111e-05 | norm: 1.6327 | dt: 4003.00ms | tokens/sec: 65486.89\n","epoch 3 | total_steps 280 | loss: 1.073763 | lr 5.0355e-05 | norm: 1.6741 | dt: 4002.30ms | tokens/sec: 65498.36\n","epoch 3 | total_steps 290 | loss: 1.331419 | lr 4.9575e-05 | norm: 1.4674 | dt: 4015.52ms | tokens/sec: 65282.74\n","epoch 3 | total_steps 300 | loss: 0.978372 | lr 4.8773e-05 | norm: 1.5615 | dt: 4006.18ms | tokens/sec: 65434.86\n","epoch 3 | total_steps 310 | loss: 0.904179 | lr 4.7948e-05 | norm: 1.8573 | dt: 4004.67ms | tokens/sec: 65459.60\n","epoch 3 | total_steps 320 | loss: 1.237903 | lr 4.7103e-05 | norm: 1.4627 | dt: 4011.72ms | tokens/sec: 65344.50\n","epoch 3 | total_steps 330 | loss: 0.770297 | lr 4.6239e-05 | norm: 1.7424 | dt: 4002.56ms | tokens/sec: 65494.03\n","epoch 3 | total_steps 340 | loss: 1.201039 | lr 4.5356e-05 | norm: 1.3635 | dt: 4002.03ms | tokens/sec: 65502.78\n","epoch 3 | total_steps 350 | loss: 0.985586 | lr 4.4456e-05 | norm: 1.4100 | dt: 4002.15ms | tokens/sec: 65500.73\n","Shuffling validation buffers\n","End of epoch 3 | Avg Train Loss: 1.117458 | Avg Val Loss: 1.915903\n","Shuffling training buffers\n","epoch 4 | total_steps 360 | loss: 1.193462 | lr 4.3541e-05 | norm: 1.8947 | dt: 4003.06ms | tokens/sec: 65485.94\n","epoch 4 | total_steps 370 | loss: 1.342579 | lr 4.2610e-05 | norm: 1.5020 | dt: 4000.28ms | tokens/sec: 65531.45\n","epoch 4 | total_steps 380 | loss: 1.161877 | lr 4.1667e-05 | norm: 1.4157 | dt: 4000.73ms | tokens/sec: 65524.05\n","epoch 4 | total_steps 390 | loss: 1.119657 | lr 4.0712e-05 | norm: 1.6620 | dt: 4004.19ms | tokens/sec: 65467.38\n","epoch 4 | total_steps 400 | loss: 0.811691 | lr 3.9746e-05 | norm: 1.4220 | dt: 4001.66ms | tokens/sec: 65508.89\n","epoch 4 | total_steps 410 | loss: 0.973230 | lr 3.8770e-05 | norm: 1.4429 | dt: 4003.23ms | tokens/sec: 65483.07\n","epoch 4 | total_steps 420 | loss: 1.251840 | lr 3.7787e-05 | norm: 1.4155 | dt: 3998.94ms | tokens/sec: 65553.30\n","epoch 4 | total_steps 430 | loss: 0.786819 | lr 3.6797e-05 | norm: 1.4255 | dt: 3996.74ms | tokens/sec: 65589.45\n","epoch 4 | total_steps 440 | loss: 1.101467 | lr 3.5802e-05 | norm: 1.5517 | dt: 3999.94ms | tokens/sec: 65536.96\n","Shuffling validation buffers\n","End of epoch 4 | Avg Train Loss: 1.011132 | Avg Val Loss: 1.986141\n","Shuffling training buffers\n","epoch 5 | total_steps 450 | loss: 0.717108 | lr 3.4803e-05 | norm: 1.2148 | dt: 4003.08ms | tokens/sec: 65485.56\n","epoch 5 | total_steps 460 | loss: 0.695783 | lr 3.3802e-05 | norm: 1.2558 | dt: 4002.77ms | tokens/sec: 65490.69\n","epoch 5 | total_steps 470 | loss: 1.181015 | lr 3.2799e-05 | norm: 1.4566 | dt: 4003.08ms | tokens/sec: 65485.61\n","epoch 5 | total_steps 480 | loss: 1.098912 | lr 3.1797e-05 | norm: 1.5304 | dt: 4007.86ms | tokens/sec: 65407.41\n","epoch 5 | total_steps 490 | loss: 0.953802 | lr 3.0797e-05 | norm: 1.1351 | dt: 4006.34ms | tokens/sec: 65432.32\n","epoch 5 | total_steps 500 | loss: 0.771352 | lr 2.9799e-05 | norm: 1.2705 | dt: 4006.25ms | tokens/sec: 65433.71\n","epoch 5 | total_steps 510 | loss: 1.007362 | lr 2.8806e-05 | norm: 1.2768 | dt: 4002.75ms | tokens/sec: 65490.96\n","epoch 5 | total_steps 520 | loss: 1.135958 | lr 2.7819e-05 | norm: 1.2190 | dt: 4003.62ms | tokens/sec: 65476.74\n","epoch 5 | total_steps 530 | loss: 0.843271 | lr 2.6838e-05 | norm: 1.1288 | dt: 4001.65ms | tokens/sec: 65509.03\n","Shuffling validation buffers\n","End of epoch 5 | Avg Train Loss: 0.919834 | Avg Val Loss: 2.098486\n","Shuffling training buffers\n","epoch 6 | total_steps 540 | loss: 0.751294 | lr 2.5867e-05 | norm: 1.1603 | dt: 4003.81ms | tokens/sec: 65473.56\n","epoch 6 | total_steps 550 | loss: 0.797663 | lr 2.4905e-05 | norm: 1.2044 | dt: 4004.76ms | tokens/sec: 65458.15\n","epoch 6 | total_steps 560 | loss: 0.728173 | lr 2.3954e-05 | norm: 1.1626 | dt: 4002.23ms | tokens/sec: 65499.44\n","epoch 6 | total_steps 570 | loss: 1.002657 | lr 2.3016e-05 | norm: 1.1206 | dt: 4004.22ms | tokens/sec: 65466.90\n","epoch 6 | total_steps 580 | loss: 0.846818 | lr 2.2091e-05 | norm: 1.1457 | dt: 4002.54ms | tokens/sec: 65494.34\n","epoch 6 | total_steps 590 | loss: 0.793537 | lr 2.1182e-05 | norm: 1.2280 | dt: 4004.80ms | tokens/sec: 65457.52\n","epoch 6 | total_steps 600 | loss: 1.072186 | lr 2.0289e-05 | norm: 1.1362 | dt: 4004.58ms | tokens/sec: 65461.02\n","epoch 6 | total_steps 610 | loss: 0.844447 | lr 1.9413e-05 | norm: 1.0470 | dt: 4003.06ms | tokens/sec: 65485.86\n","epoch 6 | total_steps 620 | loss: 0.644301 | lr 1.8556e-05 | norm: 0.9974 | dt: 3999.96ms | tokens/sec: 65536.68\n","Shuffling validation buffers\n","End of epoch 6 | Avg Train Loss: 0.842233 | Avg Val Loss: 2.174558\n","Shuffling training buffers\n","epoch 7 | total_steps 630 | loss: 0.832956 | lr 1.7719e-05 | norm: 1.0794 | dt: 4003.44ms | tokens/sec: 65479.71\n","epoch 7 | total_steps 640 | loss: 0.811460 | lr 1.6904e-05 | norm: 1.1102 | dt: 4003.46ms | tokens/sec: 65479.36\n","epoch 7 | total_steps 650 | loss: 0.445437 | lr 1.6110e-05 | norm: 0.9948 | dt: 4003.27ms | tokens/sec: 65482.47\n","epoch 7 | total_steps 660 | loss: 0.999128 | lr 1.5339e-05 | norm: 1.0742 | dt: 4003.23ms | tokens/sec: 65483.12\n","epoch 7 | total_steps 670 | loss: 0.569780 | lr 1.4593e-05 | norm: 1.0404 | dt: 4004.55ms | tokens/sec: 65461.48\n","epoch 7 | total_steps 680 | loss: 0.813017 | lr 1.3873e-05 | norm: 1.0207 | dt: 4002.69ms | tokens/sec: 65492.01\n","epoch 7 | total_steps 690 | loss: 0.896513 | lr 1.3178e-05 | norm: 1.0214 | dt: 4001.88ms | tokens/sec: 65505.14\n","epoch 7 | total_steps 700 | loss: 1.029779 | lr 1.2511e-05 | norm: 0.9898 | dt: 4003.35ms | tokens/sec: 65481.24\n","epoch 7 | total_steps 710 | loss: 0.964701 | lr 1.1873e-05 | norm: 0.9701 | dt: 4004.40ms | tokens/sec: 65463.97\n","Shuffling validation buffers\n","End of epoch 7 | Avg Train Loss: 0.781531 | Avg Val Loss: 2.247471\n","Shuffling training buffers\n","epoch 8 | total_steps 720 | loss: 0.716599 | lr 1.1263e-05 | norm: 1.1178 | dt: 4003.19ms | tokens/sec: 65483.83\n","epoch 8 | total_steps 730 | loss: 0.639333 | lr 1.0683e-05 | norm: 0.8836 | dt: 4002.39ms | tokens/sec: 65496.86\n","epoch 8 | total_steps 740 | loss: 0.779851 | lr 1.0135e-05 | norm: 0.9235 | dt: 4003.14ms | tokens/sec: 65484.61\n","epoch 8 | total_steps 750 | loss: 0.843764 | lr 9.6173e-06 | norm: 0.9542 | dt: 4002.80ms | tokens/sec: 65490.13\n","epoch 8 | total_steps 760 | loss: 0.751444 | lr 9.1322e-06 | norm: 0.9612 | dt: 4002.42ms | tokens/sec: 65496.32\n","epoch 8 | total_steps 770 | loss: 0.823315 | lr 8.6801e-06 | norm: 1.0039 | dt: 4003.04ms | tokens/sec: 65486.27\n","epoch 8 | total_steps 780 | loss: 0.572883 | lr 8.2614e-06 | norm: 0.9023 | dt: 4001.91ms | tokens/sec: 65504.78\n","epoch 8 | total_steps 790 | loss: 0.847414 | lr 7.8769e-06 | norm: 0.9085 | dt: 4002.93ms | tokens/sec: 65488.05\n","epoch 8 | total_steps 800 | loss: 0.787812 | lr 7.5270e-06 | norm: 0.8928 | dt: 4001.83ms | tokens/sec: 65506.01\n","Shuffling validation buffers\n","End of epoch 8 | Avg Train Loss: 0.740182 | Avg Val Loss: 2.326985\n","Shuffling training buffers\n","epoch 9 | total_steps 810 | loss: 0.859811 | lr 7.2122e-06 | norm: 0.8385 | dt: 4002.97ms | tokens/sec: 65487.38\n","epoch 9 | total_steps 820 | loss: 0.524289 | lr 6.9330e-06 | norm: 0.8187 | dt: 3999.23ms | tokens/sec: 65548.70\n","epoch 9 | total_steps 830 | loss: 0.754792 | lr 6.6898e-06 | norm: 0.8735 | dt: 4003.15ms | tokens/sec: 65484.36\n","epoch 9 | total_steps 840 | loss: 0.883750 | lr 6.4828e-06 | norm: 0.9666 | dt: 4002.16ms | tokens/sec: 65500.62\n","epoch 9 | total_steps 850 | loss: 0.739541 | lr 6.3123e-06 | norm: 0.9290 | dt: 4003.42ms | tokens/sec: 65479.95\n","epoch 9 | total_steps 860 | loss: 0.518593 | lr 6.1787e-06 | norm: 0.8140 | dt: 4003.01ms | tokens/sec: 65486.80\n","epoch 9 | total_steps 870 | loss: 0.678129 | lr 6.0821e-06 | norm: 0.8472 | dt: 4004.93ms | tokens/sec: 65455.27\n","epoch 9 | total_steps 880 | loss: 0.769811 | lr 6.0225e-06 | norm: 0.8926 | dt: 4005.23ms | tokens/sec: 65450.45\n","epoch 9 | total_steps 890 | loss: 0.717194 | lr 6.0002e-06 | norm: 0.8265 | dt: 4001.58ms | tokens/sec: 65510.19\n","Shuffling validation buffers\n","End of epoch 9 | Avg Train Loss: 0.712154 | Avg Val Loss: 2.353230\n"]}]},{"cell_type":"code","source":["# attempt to autodetect the device\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","    device = \"mps\"\n","print(f\"using device: {device}\")\n","\n","torch.manual_seed(1337)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(1337)\n","\n","model = GPT(GPTConfig(vocab_size=50304))\n","\n","model.to(device)\n","model = torch.compile(model)\n","model.load_state_dict(torch.load('model_medium_chat_HF_final.pth'), strict=False)\n","model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuXEmH6H0jCx","executionInfo":{"status":"ok","timestamp":1724763091191,"user_tz":-60,"elapsed":42137,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"6286dc28-162c-4271-f83e-289ce84ed735"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-7b6ab1724144>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('model_medium_chat_HF_final.pth'), strict=False)\n"]},{"output_type":"execute_result","data":{"text/plain":["OptimizedModule(\n","  (_orig_mod): GPT(\n","    (transformer): ModuleDict(\n","      (wte): Embedding(50304, 1024)\n","      (wpe): Embedding(1024, 1024)\n","      (h): ModuleList(\n","        (0-23): 24 x Block(\n","          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attn): CausalSelfAttention(\n","            (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n","            (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (mlp): MLP(\n","            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n","            (gelu): GELU(approximate='tanh')\n","            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import torch\n","\n","def generate_text(model, start_text, max_length=50, temperature=1.0, top_k=None):\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Tokenize the starting text\n","    enc = tiktoken.get_encoding('gpt2')\n","    input_ids = enc.encode(start_text, allowed_special={'<|endoftext|>'})\n","    input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n","\n","    generated_text = start_text\n","\n","    with torch.no_grad():\n","        for _ in range(max_length):\n","            outputs = model(input_ids)\n","            logits = outputs[0][:, -1, :]  # Get logits of the last token\n","\n","            # Apply temperature and top-k sampling\n","            logits = logits / temperature\n","            if top_k is not None:\n","                logits = top_k_logits(logits, top_k)\n","\n","            probs = torch.nn.functional.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            next_token_id = next_token.item()\n","\n","            # Add the predicted token to the sequence\n","            input_ids = torch.cat((input_ids, next_token), dim=1)\n","            generated_text += enc.decode([next_token_id])\n","\n","            # Stop if end-of-text token is generated\n","            if next_token_id == enc.eot_token:\n","                break\n","\n","    return generated_text\n","\n","def generate_chat_text(model, start_text, max_length=50, temperature=1.0, top_k=None):\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Tokenize the starting text\n","    enc = tiktoken.get_encoding('gpt2')\n","    input_ids = enc.encode(start_text, allowed_special={'<|endoftext|>'})\n","    input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n","\n","    generated_text = start_text\n","\n","    with torch.no_grad():\n","        for i in range(max_length):\n","            outputs = model(input_ids)\n","            logits = outputs[0][:, -1, :]  # Get logits of the last token\n","\n","            # Apply temperature and top-k sampling\n","            logits = logits / temperature\n","            if top_k is not None:\n","                logits = top_k_logits(logits, top_k)\n","\n","            probs = torch.nn.functional.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            next_token_id = next_token.item()\n","\n","            # Add the predicted token to the sequence\n","            input_ids = torch.cat((input_ids, next_token), dim=1)\n","            generated_text += enc.decode([next_token_id])\n","\n","            # stop if a question or answer prompt generated\n","            if i >= 2 and (generated_text[-2:] == 'Q:' or generated_text[-2:] == 'A:'):\n","                generated_text = generated_text[:-3]\n","                break\n","\n","            # Stop if end-of-text token is generated\n","            if next_token_id == enc.eot_token:\n","                break\n","\n","    return generated_text\n","\n","def top_k_logits(logits, k):\n","    \"\"\" Set all logits but the k highest to -infinity \"\"\"\n","    values, indices = torch.topk(logits, k)\n","    out = logits.clone()\n","    out[out < values[..., -1, None]] = -float('Inf')\n","    return out"],"metadata":{"id":"1lvgX6EIW9Js","executionInfo":{"status":"ok","timestamp":1724763101134,"user_tz":-60,"elapsed":1068,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","\n","def test():\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Tokenize the starting text\n","    tokenizer = tiktoken.get_encoding('gpt2')\n","\n","    generated = ''\n","\n","    while True:\n","        try:\n","            input_text = input(\"> \")\n","            if input_text == 'exit':\n","                print(\"-------------------------------------------\")\n","                print(generated)\n","                print(\"-------------------------------------------\")\n","                break\n","            initial_length = len(generated)\n","            generated += \"\\nQ: \" + input_text + \"\\nA:\"\n","            new_length = len(generated)\n","            statement_length = new_length - initial_length\n","\n","\n","            if new_length > 1024:\n","                generated = generated[-1024:]\n","                initial_length = 1024 - statement_length\n","\n","\n","            generated = generate_chat_text(model, generated, max_length=100, temperature=1.0, top_k=50)\n","\n","            print(generated[initial_length:].replace('<|endoftext|>','\\n'))\n","\n","        except Exception as e:\n","            print(e)\n","\n","test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jSsNEAjLWDqj","outputId":"fd9a4049-60c6-45d1-d352-e7d0aac7a496","executionInfo":{"status":"error","timestamp":1724765353368,"user_tz":-60,"elapsed":273960,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["> Hello, who am i talking to?\n","before new_length=34\n","after len(generated)=34\n","\n","Q: Hello, who am i talking to?\n","A: I can not hear you.\n","> Can you hear me now?\n","before new_length=81\n","after len(generated)=81\n","\n","Q: Can you hear me now?\n","A: Yes, sure where are you from?\n","> I am planning a trip to Cambridge.\n","before new_length=152\n","after len(generated)=152\n","\n","Q: I am planning a trip to Cambridge.\n","A: I have found a flight with Alaska Airlines it leaves at 6:40 pm and Best Western Plus.\n","> What airport doe it arrive at?\n","before new_length=276\n","after len(generated)=276\n","\n","Q: What airport doe it arrive at?\n","A: August 31st February 7 through 23rd.\n","> And is that close to Cambridge?\n","before new_length=351\n","after len(generated)=351\n","\n","Q: And is that close to Cambridge?\n","A: Sorry, did you say that there's no layover?\n","> Is it easy to get to Cambridge from there?\n","before new_length=444\n","after len(generated)=444\n","\n","Q: Is it easy to get to Cambridge from there?\n","A: sorry there are no trains departing from Birmingham New Zealand.\n","> Can I book a table for 3 tomorrow eveninf at 8pm?\n","before new_length=565\n","after len(generated)=565\n","\n","Q: Can I book a table for 3 tomorrow eveninf at 8pm?\n","A: Looks like I have so this is available. Where would you like your seating?\n","> Can I confirm the booking?\n","before new_length=673\n","after len(generated)=673\n","\n","Q: Can I confirm the booking?\n","A: Sure. Is there anything else I can help you with?\n","> Yes, can you tell me about the Cambridge night lifeplease.\n","before new_length=788\n","after len(generated)=788\n","\n","Q: Yes, can you tell me about the Cambridge night lifeplease.\n","A: It is unavailable for that time.\n","> How many cinemas are there in Cambridge?\n","before new_length=868\n","after len(generated)=868\n","\n","Q: How many cinemas are there in Cambridge?\n","A: Many hotels have openings including the price range, and the price would be $230.\n","> Is there a library?\n","before new_length=976\n","after len(generated)=976\n","\n","Q: Is there a library?\n","A: Not in the west.\n","> But what about in the east of cambridge?\n","before new_length=1040\n","after len(generated)=1024\n","\n","Q: But what about in the east of cambridge?\n","A: Just one moment. There is museum 2nd was shown in anglia rd, one is called tritl did Itdel 7 work?\n","> Could you tell me about the museum?\n","before new_length=1165\n","after len(generated)=1024\n","\n","Q: Could you tell me about the museum?\n","A: Corpus fol systematically, yes. Admission is 5berry Churchurs, the posto is at kingfajanta Department, refearsfast packed, halibars & farmline restaurants, A ont nicely salad baratedepellow (gald down for requk), and dignify by lodge salads for stock,only any other questions before heroic breaks?\n","> jfgasiludg\n","before new_length=1339\n","after len(generated)=1024\n","\n","Q: jfgasiludg\n","A: Ok what is your preference on the restaurant Can i help you with today?\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-86c48dc5c756>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-86c48dc5c756>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNA1kymnMtwc1w0NmS79URP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
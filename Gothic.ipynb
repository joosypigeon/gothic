{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dff8d25-fe6e-40c0-b86a-622e9b2ddbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Castle of Otranto already downloaded for author Walpole\n",
      "The Mysteries of Udolpho already downloaded for author Radcliffe\n",
      "Failed to download The Italian for author Radcliffe\n",
      "The Monk already downloaded for author Lewis\n",
      "Frankenstein already downloaded for author Shelley\n",
      "Dracula already downloaded for author Stoker\n",
      "Dracula's Guest already downloaded for author Stoker\n",
      "Wuthering Heights already downloaded for author E_Bronte\n",
      "Jane Eyre already downloaded for author C_Bronte\n",
      "Melmoth the Wanderer already downloaded for author Maturin\n",
      "The Works of Edgar Allan Poe already downloaded for author Poe\n",
      "The House of the Seven Gables already downloaded for author Hawthorne\n",
      "Uncle Silas already downloaded for author Le_Fanu\n",
      "Carmilla already downloaded for author Le_Fanu\n",
      "Download complete!\n",
      "Found 1 files in directory: ./gothic_novels/Walpole\n",
      "Found 1 files in directory: ./gothic_novels/Radcliffe\n",
      "Found 1 files in directory: ./gothic_novels/Lewis\n",
      "Found 1 files in directory: ./gothic_novels/Shelley\n",
      "Found 2 files in directory: ./gothic_novels/Stoker\n",
      "Found 1 files in directory: ./gothic_novels/E_Bronte\n",
      "Found 1 files in directory: ./gothic_novels/C_Bronte\n",
      "Found 1 files in directory: ./gothic_novels/Maturin\n",
      "Found 1 files in directory: ./gothic_novels/Poe\n",
      "Found 1 files in directory: ./gothic_novels/Hawthorne\n",
      "Found 2 files in directory: ./gothic_novels/Le_Fanu\n",
      "Gothic novels dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Define authors and their corresponding Gutenberg URLs\n",
    "authors_and_books = {\n",
    "    \"Walpole\": [\n",
    "        (\"The Castle of Otranto\", \"https://www.gutenberg.org/files/696/696-0.txt\")\n",
    "    ],\n",
    "    \"Radcliffe\": [\n",
    "        (\"The Mysteries of Udolpho\", \"https://www.gutenberg.org/files/3268/3268-0.txt\"),\n",
    "        (\"The Italian\", \"https://www.gutenberg.org/files/7881/7881-0.txt\")\n",
    "    ],\n",
    "    \"Lewis\": [\n",
    "        (\"The Monk\", \"https://www.gutenberg.org/files/601/601-0.txt\")\n",
    "    ],\n",
    "    \"Shelley\": [\n",
    "        (\"Frankenstein\", \"https://www.gutenberg.org/files/84/84-0.txt\")\n",
    "    ],\n",
    "    \"Stoker\": [\n",
    "        (\"Dracula\", \"https://www.gutenberg.org/files/345/345-0.txt\"),\n",
    "        (\"Dracula's Guest\", \"https://www.gutenberg.org/files/564/564-0.txt\")\n",
    "    ],\n",
    "    \"E_Bronte\": [\n",
    "        (\"Wuthering Heights\", \"https://www.gutenberg.org/files/768/768-0.txt\")\n",
    "    ],\n",
    "    \"C_Bronte\": [\n",
    "        (\"Jane Eyre\", \"https://www.gutenberg.org/files/1260/1260-0.txt\")\n",
    "    ],\n",
    "    \"Maturin\": [\n",
    "        (\"Melmoth the Wanderer\", \"https://www.gutenberg.org/files/15859/15859-0.txt\")\n",
    "    ],\n",
    "    \"Poe\": [\n",
    "        (\"The Works of Edgar Allan Poe\", \"https://www.gutenberg.org/files/2147/2147-0.txt\")\n",
    "    ],\n",
    "    \"Hawthorne\": [\n",
    "        (\"The House of the Seven Gables\", \"https://www.gutenberg.org/files/77/77-0.txt\")\n",
    "    ],\n",
    "    \"Le_Fanu\": [\n",
    "        (\"Uncle Silas\", \"https://www.gutenberg.org/files/14851/14851-0.txt\"),\n",
    "        (\"Carmilla\", \"https://www.gutenberg.org/files/10007/10007-0.txt\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "base_path = \"./gothic_novels\"\n",
    "\n",
    "# Create directories for each author\n",
    "for author in authors_and_books.keys():\n",
    "    author_path = os.path.join(base_path, author)\n",
    "    os.makedirs(author_path, exist_ok=True)\n",
    "\n",
    "# Download text files using requests\n",
    "for author, books in authors_and_books.items():\n",
    "    for book_title, book_url in books:\n",
    "        book_path = os.path.join(base_path, author, f\"{book_title}.txt\")\n",
    "        if not os.path.exists(book_path):\n",
    "            response = requests.get(book_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(book_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                print(f\"Downloaded {book_title} for author {author}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {book_title} for author {author}\")\n",
    "        else:\n",
    "            print(f\"{book_title} already downloaded for author {author}\")\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# Verify that directories and files exist\n",
    "for author in authors_and_books.keys():\n",
    "    author_path = os.path.join(base_path, author)\n",
    "    if not os.path.exists(author_path):\n",
    "        print(f\"Directory does not exist: {author_path}\")\n",
    "    else:\n",
    "        files = os.listdir(author_path)\n",
    "        if not files:\n",
    "            print(f\"No files found in directory: {author_path}\")\n",
    "        else:\n",
    "            print(f\"Found {len(files)} files in directory: {author_path}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove Gutenberg headers/footers\n",
    "    start_pattern = r'\\*\\*\\* START OF [^\\*]* \\*\\*\\*'\n",
    "    end_pattern = r'\\*\\*\\* END OF [^\\*]* \\*\\*\\*'\n",
    "    start_match = re.search(start_pattern, text)\n",
    "    end_match = re.search(end_pattern, text)\n",
    "    if start_match and end_match:\n",
    "        text = text[start_match.end():end_match.start()]\n",
    "    # Additional cleaning if necessary\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove non-ascii characters\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Aggregate texts\n",
    "all_texts = []\n",
    "for author in authors_and_books.keys():\n",
    "    author_path = os.path.join(base_path, author)\n",
    "    for filename in os.listdir(author_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(author_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                cleaned_text = clean_text(text)\n",
    "                all_texts.append(cleaned_text)\n",
    "\n",
    "# Combine texts into a single file\n",
    "combined_texts = \"\\n\\n\".join(all_texts)\n",
    "with open(\"gothic_novels_combined.txt\", 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(combined_texts)\n",
    "\n",
    "print(\"Gothic novels dataset created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeda876-8bbd-44bf-beb7-7071e8a9a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014c208191c4af18d39fb5ebfccd336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/jerry/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='11487' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  142/11487 05:29 < 7:24:21, 0.43 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import gc\n",
    "\n",
    "# Function to free GPU memory\n",
    "def free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Free GPU memory before starting\n",
    "free_gpu_memory()\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'gpt2-medium'  # or your chosen model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and prepare the dataset\n",
    "with open('gothic_novels_combined.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split the text into smaller chunks if needed\n",
    "texts = [data[i:i + 2048] for i in range(0, len(data), 2048)]\n",
    "\n",
    "# Create a dataset from the text chunks\n",
    "dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "split_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "# Prepare LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, lora_config).to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Corrected to evaluation_strategy\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    no_cuda=not torch.cuda.is_available(),  # Ensure correct device usage\n",
    ")\n",
    "\n",
    "# Data Collator to handle padding\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-model')\n",
    "\n",
    "# Free GPU memory after training\n",
    "free_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e84644-e5c8-411c-8c71-801fed362acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './fine-tuned-model'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_texts = [tokenizer.decode(output_seq, skip_special_tokens=True) for output_seq in output]\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a midnight dreary\"\n",
    "generated_texts = generate_text(prompt, max_length=200, num_return_sequences=3)\n",
    "\n",
    "# Print the generated texts\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\\n{text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

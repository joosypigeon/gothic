{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RoqGhlx-JTI2"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"foqbCnfgJTI9"},"source":["## Quick Test"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"ID4rtb6iJTJF"},"source":["## GPT2 Tokenizer"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Aq42sVNSJTJI"},"source":["## PyTorch Datasets & Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1724788744725,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"PEYOgHOYJTJI","outputId":"cc138437-4994-47c6-f8c8-72a14446dc4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/gothic\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","\n","# GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems.\n","batch_size = 16\n","max_length = 1024  # maximum sentence length\n","\n","class GothicDataset(Dataset):\n","\n","    def __init__(self, tokenizer, max_length=1024):\n","        self.tokenizer = tokenizer\n","\n","\n","        # Load the tokens and split them into training and validation sets\n","        with open('./gothic_novels_combined.txt', 'r') as f:\n","            text = f.read()\n","        print(f\"loaded {len(text)} characters\")\n","\n","        encodings_dict = tokenizer(text)\n","\n","        print(f\"{type(encodings_dict['input_ids'])}\")\n","        print(f\"{type(encodings_dict['attention_mask'])}\")\n","\n","        self.input_ids= encodings_dict['input_ids']\n","        self.attn_masks = encodings_dict['attention_mask']\n","\n","        print(f\"{len(self.input_ids)=}\")\n","        print(f\"{len(self.attn_masks)=}\")\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]\n","\n","\n","dataset = GothicDataset(tokenizer, max_length=1024)\n","\n","# Split into training and validation sets\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))\n","\n","# Create the DataLoaders for our training and validation datasets.\n","# We'll take training samples in random order.\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fXQBraz6JTJK"},"source":["## Finetune GPT2 Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":5820,"status":"error","timestamp":1724793476451,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"U0-IkTIeJTJK","outputId":"079249de-191d-4e93-cfd9-d7e0d8098321"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n","Gradient accumulation steps: 32\n"]},{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9bb209326839>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGothicDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Split into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW, get_linear_schedule_with_warmup\n","import time\n","import datetime\n","import numpy as np\n","import random\n","\n","# Parameters\n","batch_size = 16  # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","class GothicDataset(Dataset):\n","\n","    def __init__(self, tokenizer, max_length=1024):\n","        self.tokenizer = tokenizer\n","\n","\n","        # Load the tokens and split them into training and validation sets\n","        with open('./gothic_novels_combined.txt', 'r') as f:\n","            text = f.read()\n","        print(f\"loaded {len(text)} characters\")\n","\n","        encodings_dict = tokenizer(text)\n","\n","        print(f\"{type(encodings_dict['input_ids'])}\")\n","        print(f\"{type(encodings_dict['attention_mask'])}\")\n","\n","        self.input_ids= encodings_dict['input_ids']\n","        self.attn_masks = encodings_dict['attention_mask']\n","\n","        print(f\"{len(self.input_ids)=}\")\n","        print(f\"{len(self.attn_masks)=}\")\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]\n","\n","\n","dataset = GothicDataset(tokenizer, max_length=1024)\n","\n","# Split into training and validation sets\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))\n","\n","# Create the DataLoaders for our training and validation datasets.\n","# We'll take training samples in random order.\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":12008070,"status":"ok","timestamp":1724810056878,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"hMUe274RJTJM","outputId":"64d400eb-6ff8-4b2a-b49a-6c05e40f739f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n","Gradient accumulation steps: 256\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 66732716 characters\n"]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (17125777 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["Number of input sequences: 16724\n","15,051 training samples\n","1,673 validation samples\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","\n","  Average training loss: 0.01\n","  Training epoch took: 0:33:22\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 3.05\n","  Validation took: 0:05:51\n","\n","Epoch Summary:\n","  Epoch 1 / 5\n","  Training Loss: 0.01\n","  Validation Loss: 3.05\n","  Training Time: 0:33:22\n","  Validation Time: 0:05:51\n","\n","Generating Sample Output...\n","0:  vehicle and then turn. The next instant he felt the wind and his skin. He opened his mouth to breathe but it was as cold as the air. He began to rise, to walk again, and now he was in his brother's bed. He sat down, and his face was pale. He was conscious that he had begun to sneeze. But it was too late. His skin was cold as that and had the same effect on his heart. He stood up, and fell back on the pillow. Then he stood up and went slowly to the bed.\n","\n","\"How cold was it?\" he asked.\n","\n","\"What was it?\" his brother said.\n","\n","\"I'm not so cold as when I slept; it just stings a bit. What was it?\"\n","\n","\"The fever. I don't feel all that bad myself; but it's more than that; I think you know I've got it for some time. Have I been ill?\"\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","\n","  Average training loss: 0.01\n","  Training epoch took: 0:33:21\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 3.22\n","  Validation took: 0:05:51\n","\n","Epoch Summary:\n","  Epoch 2 / 5\n","  Training Loss: 0.01\n","  Validation Loss: 3.22\n","  Training Time: 0:33:21\n","  Validation Time: 0:05:51\n","\n","Generating Sample Output...\n","0:  crazy. I'm not saying the earth was all one world. But I think it is a world not to be explored; not to know all\n","the things that I know. This is a world that has been made by a supernatural power; and\n","in my opinion, it has not been explored; because if there is anything in this world that can\n","cause sickness, it has been here. Every kind of sickness that has ever been seen has originated\n","in the country of Israel; for there are more cases of these kinds of things, and I have seen them\n","myself. And I am here; and I see nothing at all about the sickness. And I have seen so many\n","things that were never reported before--things that I cannot repeat to you; things that I could never have told you\n","before, except that my friends wrote to me, and I have had to come and see them, and I can't tell you the truth;\n","but I do know, and\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","\n","  Average training loss: 0.02\n","  Training epoch took: 0:33:23\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 4.62\n","  Validation took: 0:05:51\n","\n","Epoch Summary:\n","  Epoch 3 / 5\n","  Training Loss: 0.02\n","  Validation Loss: 4.62\n","  Training Time: 0:33:23\n","  Validation Time: 0:05:51\n","\n","Generating Sample Output...\n","0:  optimistic, and were, took the most beautiful girls on the stage and saw them all as they passed across the door, and I just saw the\n","first girl.\n","\n","\"I stopped her on, and a little woman with a smile, I looked at her. She was in deep\n","silence and\n","I was in the room, it was all white: she looked at me as though I was only a picture, and was staring\n","forward, with a\n","white face, and I should have been able to turn round\n","the room without her.\n","\n","Her face looked so, too, I should\n","forwards, but I saw that it was only partially transparent: I could see her a\n","blip of pale face with a pale pink color, a gleaming sun that was going to shine upon her face; a gleaming light; I saw the face\n","through the window, as though she had been asking me to ask me to ask, which was an\n","aff\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","\n","  Average training loss: 0.02\n","  Training epoch took: 0:33:19\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 3.94\n","  Validation took: 0:05:51\n","\n","Epoch Summary:\n","  Epoch 4 / 5\n","  Training Loss: 0.02\n","  Validation Loss: 3.94\n","  Training Time: 0:33:19\n","  Validation Time: 0:05:51\n","\n","Generating Sample Output...\n","0:  politician he. we the at.\", that_ the for that_ a,\n"," all with with he, that's, by its, when I heard it to come in the window.   A little boy's a little bit. He can go home and go away, and he has a good walk.\n","\n","At last his aunt came up to call us a little child and say that the old girl was too mad!\n","\n","\"You know the big 'butler!...and we'll go away on our walks and go to-morrow. You don't know the little man's name too well, but all that you're thinking of that is not at all.       \"How do you mean, Mr.\n","        \"You never did think that.\" I never did that.\"\n","\n","\n","\n","\"We were not quite sure that we should stay or go away. Mr.\n","Weis a little tired,\"\n","and\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","\n","  Average training loss: 0.02\n","  Training epoch took: 0:33:14\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 3.72\n","  Validation took: 0:05:51\n","\n","Epoch Summary:\n","  Epoch 5 / 5\n","  Training Loss: 0.02\n","  Validation Loss: 3.72\n","  Training Time: 0:33:14\n","  Validation Time: 0:05:51\n","\n","Generating Sample Output...\n","0:  \n"," from to and of, the\n","\n"," and the I to \" the was to, from to the and the\n","her mind, to whom he had used that art of the day, that was a\n","difficulties--that they may not come back.--\"I do not want to\n","examinate on that, or at least to account for any information I may have\n","entered or been provided by the author of that particular story--that I have\n","a good deal to convey to myself.--\"\n","\n","\"It is time,\" he said, \"that I should rather say I would be somewhat\n","confused if I were to express to you of that which is\n","found, though very strange, on my account, to have been\n","told that this was so.\"\n","\n","It was true, that he had not meant to convey any\n","uncertain information, or he may have just made it himself to\n","himself.-- \"A story is told by you--\n","\n","Training complete!\n","Total training took 3:18:35 (h:mm:ss)\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Function to format elapsed time as hh:mm:ss\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# ========================================\n","#               Parameters\n","# ========================================\n","\n","# Training parameters\n","batch_size = 2 # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","checkpoint_interval = 1  # Save checkpoint every N epochs\n","step_interval = 10\n","\n","# Gradient accumulation parameters\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ========================================\n","#        Model and Tokenizer Setup\n","# ========================================\n","\n","model_name = \"gpt2-xl\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n","tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","model = model.to(device)\n","\n","# ========================================\n","#               Dataset\n","# ========================================\n","\n","class GothicDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        # Load the text\n","        with open('./gothic_novels_combined.txt', 'r') as f:\n","            text = f.read()\n","        print(f\"Loaded {len(text)} characters\")\n","\n","        # Tokenize the text into a sequence of input tokens\n","        tokenized_text = tokenizer.encode(text)\n","\n","        # Split the tokenized text into chunks of max_length\n","        self.input_ids = []\n","        self.attn_masks = []\n","\n","        for i in range(0, len(tokenized_text) - max_length + 1, max_length):\n","            chunk = tokenized_text[i:i + max_length]\n","\n","            # Create padding if needed (only needed if chunks aren't exactly max_length)\n","            padding_length = max_length - len(chunk)\n","\n","            assert padding_length == 0, f\"Padding length should be 0, but got {padding_length}\"\n","\n","            # Pad input IDs with tokenizer's pad token id if necessary\n","            padded_chunk = chunk + [tokenizer.pad_token_id] * padding_length\n","\n","            # Create an attention mask: 1 for tokens, 0 for padding\n","            attention_mask = [1] * len(chunk) + [0] * padding_length\n","\n","            self.input_ids.append(padded_chunk)\n","            self.attn_masks.append(attention_mask)\n","        print(f\"Number of input sequences: {len(self.input_ids)}\")\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.input_ids[idx]\n","        attn_mask = self.attn_masks[idx]\n","        return torch.tensor(input_ids), torch.tensor(attn_mask)\n","\n","\n","\n","# Initialize dataset and dataloaders\n","dataset = GothicDataset(tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:,} training samples')\n","print(f'{val_size:,} validation samples')\n","\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n","validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","\n","# ========================================\n","#          Optimizer and Scheduler\n","# ========================================\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n","\n","# Calculate total steps\n","total_steps = len(train_dataloader) * epochs\n","\n","# Prepare learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",")\n","\n","# Initialize mixed precision training scaler with updated API\n","scaler = GradScaler()\n","\n","# ========================================\n","#          Training and Validation\n","# ========================================\n","\n","training_stats = []\n","initial_t0 = time.time()  # Measure total training time\n","\n","# Training loop\n","for epoch_i in range(epochs):\n","    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n","    print('Training...')\n","\n","    t0 = time.time()  # Measure epoch training time\n","    total_train_loss = 0  # Reset the total loss for this epoch\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        model.zero_grad()  # Clear any previously calculated gradients\n","\n","        # Forward pass and compute loss with mixed precision\n","        with autocast(device_type='cuda', dtype=torch.float16):\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n","\n","        total_train_loss += loss.item()  # Accumulate the loss\n","\n","        scaler.scale(loss).backward()  # Backward pass with scaled loss\n","\n","        if (step + 1) % step_interval == 0:\n","            print(f\"step:{step+1}\")\n","\n","        # Update parameters every `gradient_accumulation_steps`\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            model.zero_grad()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)  # Calculate the average loss\n","    training_time = format_time(time.time() - t0)\n","\n","    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n","    print(f\"  Training epoch took: {training_time}\")\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    print(\"\\nRunning Validation...\")\n","\n","    t0 = time.time()  # Measure validation time\n","    total_eval_loss = 0\n","    model.eval()\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss\n","\n","        total_eval_loss += loss.item()\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"  Validation took: {validation_time}\")\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Valid. Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","    print(\"\\nEpoch Summary:\")\n","    print(f\"  Epoch {epoch_i + 1} / {epochs}\")\n","    print(f\"  Training Loss: {avg_train_loss:.2f}\")\n","    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n","    print(f\"  Training Time: {training_time}\")\n","    print(f\"  Validation Time: {validation_time}\")\n","\n","    # ========================================\n","    #               Sampling\n","    # ========================================\n","    print(\"\\nGenerating Sample Output...\")\n","\n","    model.eval()\n","    sample_outputs = model.generate(\n","        bos_token_id=random.randint(1, 30000),\n","        do_sample=True,\n","        top_k=50,\n","        max_length=200,\n","        top_p=0.95,\n","        num_return_sequences=1\n","    )\n","    for i, sample_output in enumerate(sample_outputs):\n","        print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","    model.train()\n","\n","    # Save model checkpoint after each epoch\n","    if (epoch_i + 1) % checkpoint_interval == 0:\n","        checkpoint_dir = f'./checkpoint-{epoch_i + 1}'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        model.save_pretrained(checkpoint_dir)\n","        tokenizer.save_pretrained(checkpoint_dir)\n","\n","print(\"\\nTraining complete!\")\n","print(f\"Total training took {format_time(time.time() - initial_t0)} (h:mm:ss)\")\n","\n","# ========================================\n","#               Plotting\n","# ========================================\n","epochs = [x['epoch'] for x in training_stats]\n","training_loss = [x['Training Loss'] for x in training_stats]\n","validation_loss = [x['Valid. Loss'] for x in training_stats]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_loss, label='Training Loss')\n","plt.plot(epochs, validation_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":954,"referenced_widgets":["107c958794a34da9918baa24469d8bd2","6df6439d999a43519f8c5ccd6a2b6ac4","ff3d46d6b87b4290acd8428c36b985e9","bc07fe803dc24460ab0fd9870ecaf841","69e3ed9dc023401b8c3866d78ef8dd01","810d8ea78b28490882e47140b56574ea","3300727409204cd296f7f2962158357f","de7e56c734ee4a198120d164add69928","8534d1bbbba3402c9a08250eac979a9b","13ee64c77de5467486bb02d76cc9a805","b08cf6f31c5a402cad164ba01a0e02aa"]},"executionInfo":{"elapsed":35869,"status":"ok","timestamp":1724811250508,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"2dJ3TpYntHh6","outputId":"4146971d-8d77-4052-bd01-b95d0601a6d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/gothic\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"107c958794a34da9918baa24469d8bd2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["0:  brher it that it inher, is a and \n","\n","\" he would not have gone through his life so well\n","so well with himself as he went into this great work. But he did\n","not know how much he regretted the past.\n","\"I am not so certain I should,\" he said, \"but I do not wish\n","to have forgotten this important story.\"\n","\n","\"Ah, so, I do,\" he asked.\n","\n","\"I know why I am so glad he will. We are very fortunate to have these tales\n","as soon as we have seen them.\"\n","\n","He went and opened the box, and, as he went up and down, he found the\n","little box with the ring in it.\n","\"I understand,\" he said, \"I can go and get it, but I may\n","never come back. I should rather lose\n","one moment's health, rather than one moment's\n","satisfaction, was well compensated for the\n"]},{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1600)\n","    (wpe): Embedding(1024, 1600)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-47): 48 x GPT2Block(\n","        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint-5\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint-5\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","model.eval()\n","sample_outputs = model.generate(\n","    bos_token_id=random.randint(1, 30000),\n","    do_sample=True,\n","    top_k=50,\n","    max_length=200,\n","    top_p=0.95,\n","    num_return_sequences=1\n",")\n","for i, sample_output in enumerate(sample_outputs):\n","    print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1724810756832,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"FfBaqerZu2GE","outputId":"c4185a0a-1d2b-4e9d-8911-bd820604914e"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1d28f8a3c35946cf82e31bdbc6e3d4c9","d1df7177676f4284a42649c377c077cc","b96ffdd3cd0c475f8f9ef4faf364c040","0e2b14d92cdb4040bc82f3e021feccfb","762171df28a7413bb33fd3c3a4c8d5fe","1e0be32905c646bd8828ac56ca2587cf","c5ce02efd8854466866d56bd0841317e","a925b5e1df5244658201e7c1b78a645c","b48b80da592b4f75abe36eb3d5c866b5","947c25188a7847fa8c9d0f91094f379f","62f0057f699d4b9cb93218f521abeb00"]},"executionInfo":{"elapsed":26681,"status":"ok","timestamp":1724811773202,"user":{"displayName":"simon mullen","userId":"04641808595381754594"},"user_tz":-60},"id":"bXI4lTIUQx_J","outputId":"a6ab8d3c-9f92-40eb-e7b7-56d8e581ff05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d28f8a3c35946cf82e31bdbc6e3d4c9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tensor([[   27,    91,  9688,  1659,  5239,    91,    29,  7454,  2402,   257,\n","         15896,   288,   260,   560,    11]], device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["0: <|startoftext|>Once upon a midnight dreary, it a is\" which he himself would\n","have been very happy to be able to communicate to any of his\n","wonderings.\"\n","\n","\"This is true, for he would not have\n","wondertous to be able to communicate any of them to\n","himself. How many of them are there still,\"\n","said the old man, \"for, in one case, he\n","was very much annoyed by the whole situation. How\n","well, how much of my wishes are there left?\"\n","\n","\"Your wish, Mr. Watson, that there is one of us\n","here in the room that is a stranger.\"\n","\n","\"I wish it would be, for he could be\n","unpleasanted by us. How much\n","he would we have received from Mr. Watson, from\n","which he was very fortunate, has been my wish.\"\n","\n","\"But is that my wish,\" said the old man.\n","\n","\"Ah, then, is that we were not to have\n","wondertous intercourse with one of our persons of\n","towards whom so much interest and concern we\n","have. How glad I was able to\n","get rid of him, without\n","wondertously believing that we have an\n","appearance of friendship.\"\n","\n","\"Why, then? It's more than that, as you have said, I was too much\n","prepared. But I\n","\n","\n"]},{"data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1600)\n","    (wpe): Embedding(1024, 1600)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-47): 48 x GPT2Block(\n","        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",")"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.cuda.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint-5\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint-5\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# Prepare input prompt\n","prompt = \"<|startoftext|>Once upon a midnight dreary,\"\n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","print(generated)\n","\n","# Generate text\n","sample_outputs = model.generate(\n","                                generated,\n","                                do_sample=True,\n","                                top_k=50,\n","                                max_length=300,\n","                                top_p=0.95,\n","                                num_return_sequences=1\n","                                )\n","\n","# Display generated text\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","\n","model.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3b0a3aaad20e4bb6b547a3d3b7fd40ba","e9fbafdda379439289b6ad58b0292b33","089d94324ade4d9283acc380aa89bf3f","7ef83bbf564141139c6ded647843fcf9","dc4ea86d8e474dd1a5f1c55df6523afb","23b520f008d4487e97090b6979e48eba","f317b7e6b74d43a585977e1ed5c1d954","7dcc0f9b865d4010802ec2fd0ac15aa0","9c02701c014f4f35b15070ac7a8cc409","4c6e6c5a87d04c5db47b01f10537f6fc","affd57ec28064be99ea561909496c609"]},"id":"heATtwpnegDK","outputId":"0f604ac7-e697-41e4-cfb6-a0138ea70f15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient accumulation steps: 256\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b0a3aaad20e4bb6b547a3d3b7fd40ba","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 11118\n","    })\n","    validation: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","})\n","loading Hugging Face data\n","loading: t='train'\n","len(ds)=11118\n","loading: t='validation'\n","len(ds)=1000\n","loading: t='test'\n","len(ds)=1000\n","len(conversations)=13118\n","loading Cornell data\n","len(conversations)=221282\n","len(conversations)=234400\n","loading woz data\n","processing: t='train'\n","len(the_datasets[t])=8437\n","processing: t='dev'\n","len(the_datasets[t])=1000\n","processing: t='test'\n","len(the_datasets[t])=1000\n","len(conversations)=244837\n","loading taskmaster data\n","dir='TM-1-2019'\n","loading JSON files\n","file_path='data/Taskmaster/TM-1-2019/woz-dialogs.json'\n","file_path='data/Taskmaster/TM-1-2019/self-dialogs.json'\n","processing dialogues\n","dir='TM-2-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-2-2020/data/flights.json'\n","file_path='data/Taskmaster/TM-2-2020/data/music.json'\n","file_path='data/Taskmaster/TM-2-2020/data/hotels.json'\n","file_path='data/Taskmaster/TM-2-2020/data/restaurant-search.json'\n","file_path='data/Taskmaster/TM-2-2020/data/sports.json'\n","file_path='data/Taskmaster/TM-2-2020/data/movies.json'\n","file_path='data/Taskmaster/TM-2-2020/data/food-ordering.json'\n","processing dialogues\n","dir='TM-3-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-3-2020/data/data_15.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_05.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_18.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_09.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_16.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_02.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_14.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_07.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_00.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_04.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_03.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_01.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_11.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_12.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_10.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_19.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_13.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_17.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_08.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_06.json'\n","processing dialogues\n","dir='TM-4-2024/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-4-2024/data/data_04.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_01.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_06.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_00.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_02.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_03.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_05.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_07.json'\n","processing dialogues\n","len(conversations)=57986\n","len(conversations)=302823\n","len(text)=108960154\n"]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (31377861 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["Number of input sequences: 30642\n","27,577 training samples\n","3,065 validation samples\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-1-a38dbba471d3>:307: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-1-a38dbba471d3>:332: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(dtype=torch.float16):\n"]},{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n","step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","step:7530\n","step:7540\n","step:7550\n","step:7560\n","step:7570\n","step:7580\n","step:7590\n","step:7600\n","step:7610\n","step:7620\n","step:7630\n","step:7640\n","step:7650\n","step:7660\n","step:7670\n","step:7680\n","step:7690\n","step:7700\n","step:7710\n","step:7720\n","step:7730\n","step:7740\n","step:7750\n","step:7760\n","step:7770\n","step:7780\n","step:7790\n","step:7800\n","step:7810\n","step:7820\n","step:7830\n","step:7840\n","step:7850\n","step:7860\n","step:7870\n","step:7880\n","step:7890\n","step:7900\n","step:7910\n","step:7920\n","step:7930\n","step:7940\n","step:7950\n","step:7960\n","step:7970\n","step:7980\n","step:7990\n","step:8000\n","step:8010\n","step:8020\n","step:8030\n","step:8040\n","step:8050\n","step:8060\n","step:8070\n","step:8080\n","step:8090\n","step:8100\n","step:8110\n","step:8120\n","step:8130\n","step:8140\n","step:8150\n","step:8160\n","step:8170\n","step:8180\n","step:8190\n","step:8200\n","step:8210\n","step:8220\n","step:8230\n","step:8240\n","step:8250\n","step:8260\n","step:8270\n","step:8280\n","step:8290\n","step:8300\n","step:8310\n","step:8320\n","step:8330\n","step:8340\n","step:8350\n","step:8360\n","step:8370\n","step:8380\n","step:8390\n","step:8400\n","step:8410\n","step:8420\n","step:8430\n","step:8440\n","step:8450\n","step:8460\n","step:8470\n","step:8480\n","step:8490\n","step:8500\n","step:8510\n","step:8520\n","step:8530\n","step:8540\n","step:8550\n","step:8560\n","step:8570\n","step:8580\n","step:8590\n","step:8600\n","step:8610\n","step:8620\n","step:8630\n","step:8640\n","step:8650\n","step:8660\n","step:8670\n","step:8680\n","step:8690\n","step:8700\n","step:8710\n","step:8720\n","step:8730\n","step:8740\n","step:8750\n","step:8760\n","step:8770\n","step:8780\n","step:8790\n","step:8800\n","step:8810\n","step:8820\n","step:8830\n","step:8840\n","step:8850\n","step:8860\n","step:8870\n","step:8880\n","step:8890\n","step:8900\n","step:8910\n","step:8920\n","step:8930\n","step:8940\n","step:8950\n","step:8960\n","step:8970\n","step:8980\n","step:8990\n","step:9000\n","step:9010\n","step:9020\n","step:9030\n","step:9040\n","step:9050\n","step:9060\n","step:9070\n","step:9080\n","step:9090\n","step:9100\n","step:9110\n","step:9120\n","step:9130\n","step:9140\n","step:9150\n","step:9160\n","step:9170\n","step:9180\n","step:9190\n","step:9200\n","step:9210\n","step:9220\n","step:9230\n","step:9240\n","step:9250\n","step:9260\n","step:9270\n","step:9280\n","step:9290\n","step:9300\n","step:9310\n","step:9320\n","step:9330\n","step:9340\n","step:9350\n","step:9360\n","step:9370\n","step:9380\n","step:9390\n","step:9400\n","step:9410\n","step:9420\n","step:9430\n","step:9440\n","step:9450\n","step:9460\n","step:9470\n","step:9480\n","step:9490\n","step:9500\n","step:9510\n","step:9520\n","step:9530\n","step:9540\n","step:9550\n","step:9560\n","step:9570\n","step:9580\n","step:9590\n","step:9600\n","step:9610\n","step:9620\n","step:9630\n","step:9640\n","step:9650\n","step:9660\n","step:9670\n","step:9680\n","step:9690\n","step:9700\n","step:9710\n","step:9720\n","step:9730\n","step:9740\n","step:9750\n","step:9760\n","step:9770\n","step:9780\n","step:9790\n","step:9800\n","step:9810\n","step:9820\n","step:9830\n","step:9840\n","step:9850\n","step:9860\n","step:9870\n","step:9880\n","step:9890\n","step:9900\n","step:9910\n","step:9920\n","step:9930\n","step:9940\n","step:9950\n","step:9960\n","step:9970\n","step:9980\n","step:9990\n","step:10000\n","step:10010\n","step:10020\n","step:10030\n","step:10040\n","step:10050\n","step:10060\n","step:10070\n","step:10080\n","step:10090\n","step:10100\n","step:10110\n","step:10120\n","step:10130\n","step:10140\n","step:10150\n","step:10160\n","step:10170\n","step:10180\n","step:10190\n","step:10200\n","step:10210\n","step:10220\n","step:10230\n","step:10240\n","step:10250\n","step:10260\n","step:10270\n","step:10280\n","step:10290\n","step:10300\n","step:10310\n","step:10320\n","step:10330\n","step:10340\n","step:10350\n","step:10360\n","step:10370\n","step:10380\n","step:10390\n","step:10400\n","step:10410\n","step:10420\n","step:10430\n","step:10440\n","step:10450\n","step:10460\n","step:10470\n","step:10480\n","step:10490\n","step:10500\n","step:10510\n","step:10520\n","step:10530\n","step:10540\n","step:10550\n","step:10560\n","step:10570\n","step:10580\n","step:10590\n","step:10600\n","step:10610\n","step:10620\n","step:10630\n","step:10640\n","step:10650\n","step:10660\n","step:10670\n","step:10680\n","step:10690\n","step:10700\n","step:10710\n","step:10720\n","step:10730\n","step:10740\n","step:10750\n","step:10760\n","step:10770\n","step:10780\n","step:10790\n","step:10800\n","step:10810\n","step:10820\n","step:10830\n","step:10840\n","step:10850\n","step:10860\n","step:10870\n","step:10880\n","step:10890\n","step:10900\n","step:10910\n","step:10920\n","step:10930\n","step:10940\n","step:10950\n","step:10960\n","step:10970\n","step:10980\n","step:10990\n","step:11000\n","step:11010\n","step:11020\n","step:11030\n","step:11040\n","step:11050\n","step:11060\n","step:11070\n","step:11080\n","step:11090\n","step:11100\n","step:11110\n","step:11120\n","step:11130\n","step:11140\n","step:11150\n","step:11160\n","step:11170\n","step:11180\n","step:11190\n","step:11200\n","step:11210\n","step:11220\n","step:11230\n","step:11240\n","step:11250\n","step:11260\n","step:11270\n","step:11280\n","step:11290\n","step:11300\n","step:11310\n","step:11320\n","step:11330\n","step:11340\n","step:11350\n","step:11360\n","step:11370\n","step:11380\n","step:11390\n","step:11400\n","step:11410\n","step:11420\n","step:11430\n","step:11440\n","step:11450\n","step:11460\n","step:11470\n","step:11480\n","step:11490\n","step:11500\n","step:11510\n","step:11520\n","step:11530\n","step:11540\n","step:11550\n","step:11560\n","step:11570\n","step:11580\n","step:11590\n","step:11600\n","step:11610\n","step:11620\n","step:11630\n","step:11640\n","step:11650\n","step:11660\n","step:11670\n","step:11680\n","step:11690\n","step:11700\n","step:11710\n","step:11720\n","step:11730\n","step:11740\n","step:11750\n","step:11760\n","step:11770\n","step:11780\n","step:11790\n","step:11800\n","step:11810\n","step:11820\n","step:11830\n","step:11840\n","step:11850\n","step:11860\n","step:11870\n","step:11880\n","step:11890\n","step:11900\n","step:11910\n","step:11920\n","step:11930\n","step:11940\n","step:11950\n","step:11960\n","step:11970\n","step:11980\n","step:11990\n","step:12000\n","step:12010\n","step:12020\n","step:12030\n","step:12040\n","step:12050\n","step:12060\n","step:12070\n","step:12080\n","step:12090\n","step:12100\n","step:12110\n","step:12120\n","step:12130\n","step:12140\n","step:12150\n","step:12160\n","step:12170\n","step:12180\n","step:12190\n","step:12200\n","step:12210\n","step:12220\n","step:12230\n","step:12240\n","step:12250\n","step:12260\n","step:12270\n","step:12280\n","step:12290\n","step:12300\n","step:12310\n","step:12320\n","step:12330\n","step:12340\n","step:12350\n","step:12360\n","step:12370\n","step:12380\n","step:12390\n","step:12400\n","step:12410\n","step:12420\n","step:12430\n","step:12440\n","step:12450\n","step:12460\n","step:12470\n","step:12480\n","step:12490\n","step:12500\n","step:12510\n","step:12520\n","step:12530\n","step:12540\n","step:12550\n","step:12560\n","step:12570\n","step:12580\n","step:12590\n","step:12600\n","step:12610\n","step:12620\n","step:12630\n","step:12640\n","step:12650\n","step:12660\n","step:12670\n","step:12680\n","step:12690\n","step:12700\n","step:12710\n","step:12720\n","step:12730\n","step:12740\n","step:12750\n","step:12760\n","step:12770\n","step:12780\n","step:12790\n","step:12800\n","step:12810\n","step:12820\n","step:12830\n","step:12840\n","step:12850\n","step:12860\n","step:12870\n","step:12880\n","step:12890\n","step:12900\n","step:12910\n","step:12920\n","step:12930\n","step:12940\n","step:12950\n","step:12960\n","step:12970\n","step:12980\n","step:12990\n","step:13000\n","step:13010\n","step:13020\n","step:13030\n","step:13040\n","step:13050\n","step:13060\n","step:13070\n","step:13080\n","step:13090\n","step:13100\n","step:13110\n","step:13120\n","step:13130\n","step:13140\n","step:13150\n","step:13160\n","step:13170\n","step:13180\n","step:13190\n","step:13200\n","step:13210\n","step:13220\n","step:13230\n","step:13240\n","step:13250\n","step:13260\n","step:13270\n","step:13280\n","step:13290\n","step:13300\n","step:13310\n","step:13320\n","step:13330\n","step:13340\n","step:13350\n","step:13360\n","step:13370\n","step:13380\n","step:13390\n","step:13400\n","step:13410\n","step:13420\n","step:13430\n","step:13440\n","step:13450\n","step:13460\n","step:13470\n","step:13480\n","step:13490\n","step:13500\n","step:13510\n","step:13520\n","step:13530\n","step:13540\n","step:13550\n","step:13560\n","step:13570\n","step:13580\n","step:13590\n","step:13600\n","step:13610\n","step:13620\n","step:13630\n","step:13640\n","step:13650\n","step:13660\n","step:13670\n","step:13680\n","step:13690\n","step:13700\n","step:13710\n","step:13720\n","step:13730\n","step:13740\n","step:13750\n","step:13760\n","step:13770\n","step:13780\n","\n","  Average training loss: 0.0103\n","  Training epoch took: 1:35:10\n","\n","Running Validation...\n"]},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["  Validation Loss: 2.2483\n","  Validation took: 0:11:48\n","\n","Epoch Summary:\n","  Epoch 1 / 5\n","  Training Loss: 0.0103\n","  Validation Loss: 2.2483\n","  Training Time: 1:35:10\n","  Validation Time: 0:11:48\n","\n","Generating Sample Output...\n","0:  eighteen.\"Dreadful... Can't believe the story! This is how I felt, when I wrote the story.\n","A: But we're just a few short weeks.\n","\n","\n","======== Epoch 2 / 5 ========\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-1-a38dbba471d3>:332: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(dtype=torch.float16):\n"]},{"name":"stdout","output_type":"stream","text":["step:10\n","step:20\n","step:30\n","step:40\n","step:50\n","step:60\n","step:70\n","step:80\n","step:90\n","step:100\n","step:110\n","step:120\n","step:130\n","step:140\n","step:150\n","step:160\n","step:170\n","step:180\n","step:190\n","step:200\n","step:210\n","step:220\n","step:230\n","step:240\n","step:250\n","step:260\n","step:270\n","step:280\n","step:290\n","step:300\n","step:310\n","step:320\n","step:330\n","step:340\n","step:350\n","step:360\n","step:370\n","step:380\n","step:390\n","step:400\n","step:410\n","step:420\n","step:430\n","step:440\n","step:450\n","step:460\n","step:470\n","step:480\n","step:490\n","step:500\n","step:510\n","step:520\n","step:530\n","step:540\n","step:550\n","step:560\n","step:570\n","step:580\n","step:590\n","step:600\n","step:610\n","step:620\n","step:630\n","step:640\n","step:650\n","step:660\n","step:670\n","step:680\n","step:690\n","step:700\n","step:710\n","step:720\n","step:730\n","step:740\n","step:750\n","step:760\n","step:770\n","step:780\n","step:790\n","step:800\n","step:810\n","step:820\n","step:830\n","step:840\n","step:850\n","step:860\n","step:870\n","step:880\n","step:890\n","step:900\n","step:910\n","step:920\n","step:930\n","step:940\n","step:950\n","step:960\n","step:970\n","step:980\n","step:990\n","step:1000\n","step:1010\n","step:1020\n","step:1030\n","step:1040\n","step:1050\n","step:1060\n","step:1070\n","step:1080\n","step:1090\n","step:1100\n","step:1110\n","step:1120\n","step:1130\n","step:1140\n","step:1150\n","step:1160\n","step:1170\n","step:1180\n","step:1190\n","step:1200\n","step:1210\n","step:1220\n","step:1230\n","step:1240\n","step:1250\n","step:1260\n","step:1270\n","step:1280\n","step:1290\n","step:1300\n","step:1310\n","step:1320\n","step:1330\n","step:1340\n","step:1350\n","step:1360\n","step:1370\n","step:1380\n","step:1390\n","step:1400\n","step:1410\n","step:1420\n","step:1430\n","step:1440\n","step:1450\n","step:1460\n","step:1470\n","step:1480\n","step:1490\n","step:1500\n","step:1510\n","step:1520\n","step:1530\n","step:1540\n","step:1550\n","step:1560\n","step:1570\n","step:1580\n","step:1590\n","step:1600\n","step:1610\n","step:1620\n","step:1630\n","step:1640\n","step:1650\n","step:1660\n","step:1670\n","step:1680\n","step:1690\n","step:1700\n","step:1710\n","step:1720\n","step:1730\n","step:1740\n","step:1750\n","step:1760\n","step:1770\n","step:1780\n","step:1790\n","step:1800\n","step:1810\n","step:1820\n","step:1830\n","step:1840\n","step:1850\n","step:1860\n","step:1870\n","step:1880\n","step:1890\n","step:1900\n","step:1910\n","step:1920\n","step:1930\n","step:1940\n","step:1950\n","step:1960\n","step:1970\n","step:1980\n","step:1990\n","step:2000\n","step:2010\n","step:2020\n","step:2030\n","step:2040\n","step:2050\n","step:2060\n","step:2070\n","step:2080\n","step:2090\n","step:2100\n","step:2110\n","step:2120\n","step:2130\n","step:2140\n","step:2150\n","step:2160\n","step:2170\n","step:2180\n","step:2190\n","step:2200\n","step:2210\n","step:2220\n","step:2230\n","step:2240\n","step:2250\n","step:2260\n","step:2270\n","step:2280\n","step:2290\n","step:2300\n","step:2310\n","step:2320\n","step:2330\n","step:2340\n","step:2350\n","step:2360\n","step:2370\n","step:2380\n","step:2390\n","step:2400\n","step:2410\n","step:2420\n","step:2430\n","step:2440\n","step:2450\n","step:2460\n","step:2470\n","step:2480\n","step:2490\n","step:2500\n","step:2510\n","step:2520\n","step:2530\n","step:2540\n","step:2550\n","step:2560\n","step:2570\n","step:2580\n","step:2590\n","step:2600\n","step:2610\n","step:2620\n","step:2630\n","step:2640\n","step:2650\n","step:2660\n","step:2670\n","step:2680\n","step:2690\n","step:2700\n","step:2710\n","step:2720\n","step:2730\n","step:2740\n","step:2750\n","step:2760\n","step:2770\n","step:2780\n","step:2790\n","step:2800\n","step:2810\n","step:2820\n","step:2830\n","step:2840\n","step:2850\n","step:2860\n","step:2870\n","step:2880\n","step:2890\n","step:2900\n","step:2910\n","step:2920\n","step:2930\n","step:2940\n","step:2950\n","step:2960\n","step:2970\n","step:2980\n","step:2990\n","step:3000\n","step:3010\n","step:3020\n","step:3030\n","step:3040\n","step:3050\n","step:3060\n","step:3070\n","step:3080\n","step:3090\n","step:3100\n","step:3110\n","step:3120\n","step:3130\n","step:3140\n","step:3150\n","step:3160\n","step:3170\n","step:3180\n","step:3190\n","step:3200\n","step:3210\n","step:3220\n","step:3230\n","step:3240\n","step:3250\n","step:3260\n","step:3270\n","step:3280\n","step:3290\n","step:3300\n","step:3310\n","step:3320\n","step:3330\n","step:3340\n","step:3350\n","step:3360\n","step:3370\n","step:3380\n","step:3390\n","step:3400\n","step:3410\n","step:3420\n","step:3430\n","step:3440\n","step:3450\n","step:3460\n","step:3470\n","step:3480\n","step:3490\n","step:3500\n","step:3510\n","step:3520\n","step:3530\n","step:3540\n","step:3550\n","step:3560\n","step:3570\n","step:3580\n","step:3590\n","step:3600\n","step:3610\n","step:3620\n","step:3630\n","step:3640\n","step:3650\n","step:3660\n","step:3670\n","step:3680\n","step:3690\n","step:3700\n","step:3710\n","step:3720\n","step:3730\n","step:3740\n","step:3750\n","step:3760\n","step:3770\n","step:3780\n","step:3790\n","step:3800\n","step:3810\n","step:3820\n","step:3830\n","step:3840\n","step:3850\n","step:3860\n","step:3870\n","step:3880\n","step:3890\n","step:3900\n","step:3910\n","step:3920\n","step:3930\n","step:3940\n","step:3950\n","step:3960\n","step:3970\n","step:3980\n","step:3990\n","step:4000\n","step:4010\n","step:4020\n","step:4030\n","step:4040\n","step:4050\n","step:4060\n","step:4070\n","step:4080\n","step:4090\n","step:4100\n","step:4110\n","step:4120\n","step:4130\n","step:4140\n","step:4150\n","step:4160\n","step:4170\n","step:4180\n","step:4190\n","step:4200\n","step:4210\n","step:4220\n","step:4230\n","step:4240\n","step:4250\n","step:4260\n","step:4270\n","step:4280\n","step:4290\n","step:4300\n","step:4310\n","step:4320\n","step:4330\n","step:4340\n","step:4350\n","step:4360\n","step:4370\n","step:4380\n","step:4390\n","step:4400\n","step:4410\n","step:4420\n","step:4430\n","step:4440\n","step:4450\n","step:4460\n","step:4470\n","step:4480\n","step:4490\n","step:4500\n","step:4510\n","step:4520\n","step:4530\n","step:4540\n","step:4550\n","step:4560\n","step:4570\n","step:4580\n","step:4590\n","step:4600\n","step:4610\n","step:4620\n","step:4630\n","step:4640\n","step:4650\n","step:4660\n","step:4670\n","step:4680\n","step:4690\n","step:4700\n","step:4710\n","step:4720\n","step:4730\n","step:4740\n","step:4750\n","step:4760\n","step:4770\n","step:4780\n","step:4790\n","step:4800\n","step:4810\n","step:4820\n","step:4830\n","step:4840\n","step:4850\n","step:4860\n","step:4870\n","step:4880\n","step:4890\n","step:4900\n","step:4910\n","step:4920\n","step:4930\n","step:4940\n","step:4950\n","step:4960\n","step:4970\n","step:4980\n","step:4990\n","step:5000\n","step:5010\n","step:5020\n","step:5030\n","step:5040\n","step:5050\n","step:5060\n","step:5070\n","step:5080\n","step:5090\n","step:5100\n","step:5110\n","step:5120\n","step:5130\n","step:5140\n","step:5150\n","step:5160\n","step:5170\n","step:5180\n","step:5190\n","step:5200\n","step:5210\n","step:5220\n","step:5230\n","step:5240\n","step:5250\n","step:5260\n","step:5270\n","step:5280\n","step:5290\n","step:5300\n","step:5310\n","step:5320\n","step:5330\n","step:5340\n","step:5350\n","step:5360\n","step:5370\n","step:5380\n","step:5390\n","step:5400\n","step:5410\n","step:5420\n","step:5430\n","step:5440\n","step:5450\n","step:5460\n","step:5470\n","step:5480\n","step:5490\n","step:5500\n","step:5510\n","step:5520\n","step:5530\n","step:5540\n","step:5550\n","step:5560\n","step:5570\n","step:5580\n","step:5590\n","step:5600\n","step:5610\n","step:5620\n","step:5630\n","step:5640\n","step:5650\n","step:5660\n","step:5670\n","step:5680\n","step:5690\n","step:5700\n","step:5710\n","step:5720\n","step:5730\n","step:5740\n","step:5750\n","step:5760\n","step:5770\n","step:5780\n","step:5790\n","step:5800\n","step:5810\n","step:5820\n","step:5830\n","step:5840\n","step:5850\n","step:5860\n","step:5870\n","step:5880\n","step:5890\n","step:5900\n","step:5910\n","step:5920\n","step:5930\n","step:5940\n","step:5950\n","step:5960\n","step:5970\n","step:5980\n","step:5990\n","step:6000\n","step:6010\n","step:6020\n","step:6030\n","step:6040\n","step:6050\n","step:6060\n","step:6070\n","step:6080\n","step:6090\n","step:6100\n","step:6110\n","step:6120\n","step:6130\n","step:6140\n","step:6150\n","step:6160\n","step:6170\n","step:6180\n","step:6190\n","step:6200\n","step:6210\n","step:6220\n","step:6230\n","step:6240\n","step:6250\n","step:6260\n","step:6270\n","step:6280\n","step:6290\n","step:6300\n","step:6310\n","step:6320\n","step:6330\n","step:6340\n","step:6350\n","step:6360\n","step:6370\n","step:6380\n","step:6390\n","step:6400\n","step:6410\n","step:6420\n","step:6430\n","step:6440\n","step:6450\n","step:6460\n","step:6470\n","step:6480\n","step:6490\n","step:6500\n","step:6510\n","step:6520\n","step:6530\n","step:6540\n","step:6550\n","step:6560\n","step:6570\n","step:6580\n","step:6590\n","step:6600\n","step:6610\n","step:6620\n","step:6630\n","step:6640\n","step:6650\n","step:6660\n","step:6670\n","step:6680\n","step:6690\n","step:6700\n","step:6710\n","step:6720\n","step:6730\n","step:6740\n","step:6750\n","step:6760\n","step:6770\n","step:6780\n","step:6790\n","step:6800\n","step:6810\n","step:6820\n","step:6830\n","step:6840\n","step:6850\n","step:6860\n","step:6870\n","step:6880\n","step:6890\n","step:6900\n","step:6910\n","step:6920\n","step:6930\n","step:6940\n","step:6950\n","step:6960\n","step:6970\n","step:6980\n","step:6990\n","step:7000\n","step:7010\n","step:7020\n","step:7030\n","step:7040\n","step:7050\n","step:7060\n","step:7070\n","step:7080\n","step:7090\n","step:7100\n","step:7110\n","step:7120\n","step:7130\n","step:7140\n","step:7150\n","step:7160\n","step:7170\n","step:7180\n","step:7190\n","step:7200\n","step:7210\n","step:7220\n","step:7230\n","step:7240\n","step:7250\n","step:7260\n","step:7270\n","step:7280\n","step:7290\n","step:7300\n","step:7310\n","step:7320\n","step:7330\n","step:7340\n","step:7350\n","step:7360\n","step:7370\n","step:7380\n","step:7390\n","step:7400\n","step:7410\n","step:7420\n","step:7430\n","step:7440\n","step:7450\n","step:7460\n","step:7470\n","step:7480\n","step:7490\n","step:7500\n","step:7510\n","step:7520\n","step:7530\n","step:7540\n","step:7550\n","step:7560\n","step:7570\n","step:7580\n","step:7590\n","step:7600\n","step:7610\n","step:7620\n","step:7630\n","step:7640\n","step:7650\n","step:7660\n","step:7670\n","step:7680\n","step:7690\n","step:7700\n","step:7710\n","step:7720\n","step:7730\n","step:7740\n","step:7750\n","step:7760\n","step:7770\n","step:7780\n","step:7790\n","step:7800\n","step:7810\n","step:7820\n","step:7830\n","step:7840\n","step:7850\n","step:7860\n","step:7870\n","step:7880\n","step:7890\n","step:7900\n","step:7910\n","step:7920\n","step:7930\n","step:7940\n","step:7950\n","step:7960\n","step:7970\n","step:7980\n","step:7990\n","step:8000\n","step:8010\n","step:8020\n","step:8030\n","step:8040\n","step:8050\n","step:8060\n","step:8070\n","step:8080\n","step:8090\n","step:8100\n","step:8110\n","step:8120\n","step:8130\n","step:8140\n","step:8150\n","step:8160\n","step:8170\n","step:8180\n","step:8190\n","step:8200\n","step:8210\n","step:8220\n","step:8230\n","step:8240\n","step:8250\n","step:8260\n","step:8270\n","step:8280\n","step:8290\n","step:8300\n","step:8310\n","step:8320\n","step:8330\n","step:8340\n","step:8350\n","step:8360\n","step:8370\n","step:8380\n","step:8390\n","step:8400\n","step:8410\n","step:8420\n","step:8430\n","step:8440\n","step:8450\n","step:8460\n","step:8470\n","step:8480\n","step:8490\n","step:8500\n","step:8510\n","step:8520\n","step:8530\n","step:8540\n","step:8550\n","step:8560\n","step:8570\n","step:8580\n","step:8590\n","step:8600\n","step:8610\n","step:8620\n","step:8630\n","step:8640\n","step:8650\n","step:8660\n","step:8670\n","step:8680\n","step:8690\n","step:8700\n","step:8710\n","step:8720\n","step:8730\n","step:8740\n","step:8750\n","step:8760\n","step:8770\n","step:8780\n","step:8790\n","step:8800\n","step:8810\n","step:8820\n","step:8830\n","step:8840\n","step:8850\n","step:8860\n","step:8870\n","step:8880\n","step:8890\n","step:8900\n","step:8910\n","step:8920\n","step:8930\n","step:8940\n","step:8950\n","step:8960\n","step:8970\n","step:8980\n","step:8990\n","step:9000\n","step:9010\n","step:9020\n","step:9030\n","step:9040\n","step:9050\n","step:9060\n","step:9070\n","step:9080\n","step:9090\n","step:9100\n","step:9110\n","step:9120\n","step:9130\n","step:9140\n","step:9150\n","step:9160\n","step:9170\n","step:9180\n","step:9190\n","step:9200\n","step:9210\n","step:9220\n","step:9230\n","step:9240\n","step:9250\n","step:9260\n","step:9270\n","step:9280\n","step:9290\n","step:9300\n","step:9310\n","step:9320\n","step:9330\n","step:9340\n","step:9350\n","step:9360\n","step:9370\n","step:9380\n","step:9390\n","step:9400\n","step:9410\n","step:9420\n","step:9430\n","step:9440\n","step:9450\n","step:9460\n","step:9470\n","step:9480\n","step:9490\n","step:9500\n","step:9510\n","step:9520\n","step:9530\n","step:9540\n","step:9550\n","step:9560\n","step:9570\n","step:9580\n","step:9590\n","step:9600\n","step:9610\n","step:9620\n","step:9630\n","step:9640\n","step:9650\n","step:9660\n","step:9670\n","step:9680\n","step:9690\n","step:9700\n","step:9710\n","step:9720\n","step:9730\n","step:9740\n","step:9750\n","step:9760\n","step:9770\n","step:9780\n","step:9790\n","step:9800\n","step:9810\n","step:9820\n","step:9830\n","step:9840\n","step:9850\n","step:9860\n","step:9870\n","step:9880\n","step:9890\n","step:9900\n","step:9910\n","step:9920\n","step:9930\n","step:9940\n","step:9950\n","step:9960\n","step:9970\n","step:9980\n","step:9990\n","step:10000\n","step:10010\n","step:10020\n","step:10030\n","step:10040\n","step:10050\n","step:10060\n","step:10070\n","step:10080\n","step:10090\n","step:10100\n","step:10110\n","step:10120\n","step:10130\n","step:10140\n","step:10150\n","step:10160\n","step:10170\n","step:10180\n","step:10190\n","step:10200\n","step:10210\n","step:10220\n","step:10230\n","step:10240\n","step:10250\n","step:10260\n","step:10270\n","step:10280\n","step:10290\n","step:10300\n","step:10310\n","step:10320\n","step:10330\n","step:10340\n","step:10350\n","step:10360\n","step:10370\n","step:10380\n","step:10390\n","step:10400\n","step:10410\n","step:10420\n","step:10430\n","step:10440\n","step:10450\n","step:10460\n","step:10470\n","step:10480\n","step:10490\n","step:10500\n","step:10510\n","step:10520\n","step:10530\n","step:10540\n","step:10550\n","step:10560\n","step:10570\n","step:10580\n","step:10590\n","step:10600\n","step:10610\n","step:10620\n","step:10630\n","step:10640\n","step:10650\n","step:10660\n","step:10670\n","step:10680\n","step:10690\n","step:10700\n","step:10710\n","step:10720\n","step:10730\n","step:10740\n","step:10750\n","step:10760\n","step:10770\n","step:10780\n","step:10790\n","step:10800\n","step:10810\n","step:10820\n","step:10830\n","step:10840\n","step:10850\n","step:10860\n","step:10870\n","step:10880\n","step:10890\n","step:10900\n","step:10910\n","step:10920\n","step:10930\n","step:10940\n","step:10950\n","step:10960\n","step:10970\n","step:10980\n","step:10990\n","step:11000\n","step:11010\n","step:11020\n","step:11030\n","step:11040\n","step:11050\n","step:11060\n","step:11070\n","step:11080\n","step:11090\n","step:11100\n","step:11110\n","step:11120\n","step:11130\n","step:11140\n","step:11150\n","step:11160\n","step:11170\n","step:11180\n","step:11190\n","step:11200\n","step:11210\n","step:11220\n","step:11230\n","step:11240\n","step:11250\n","step:11260\n","step:11270\n","step:11280\n","step:11290\n","step:11300\n","step:11310\n","step:11320\n","step:11330\n","step:11340\n","step:11350\n","step:11360\n","step:11370\n","step:11380\n","step:11390\n","step:11400\n","step:11410\n","step:11420\n","step:11430\n","step:11440\n","step:11450\n","step:11460\n","step:11470\n","step:11480\n","step:11490\n","step:11500\n","step:11510\n","step:11520\n","step:11530\n","step:11540\n","step:11550\n","step:11560\n","step:11570\n","step:11580\n","step:11590\n","step:11600\n","step:11610\n","step:11620\n","step:11630\n","step:11640\n","step:11650\n","step:11660\n","step:11670\n","step:11680\n","step:11690\n","step:11700\n","step:11710\n","step:11720\n","step:11730\n","step:11740\n","step:11750\n","step:11760\n","step:11770\n","step:11780\n","step:11790\n","step:11800\n","step:11810\n","step:11820\n","step:11830\n","step:11840\n","step:11850\n","step:11860\n","step:11870\n","step:11880\n","step:11890\n","step:11900\n","step:11910\n","step:11920\n","step:11930\n","step:11940\n","step:11950\n","step:11960\n","step:11970\n","step:11980\n","step:11990\n","step:12000\n","step:12010\n","step:12020\n","step:12030\n","step:12040\n","step:12050\n","step:12060\n","step:12070\n","step:12080\n","step:12090\n","step:12100\n","step:12110\n","step:12120\n","step:12130\n","step:12140\n","step:12150\n","step:12160\n","step:12170\n","step:12180\n","step:12190\n","step:12200\n","step:12210\n","step:12220\n","step:12230\n","step:12240\n","step:12250\n","step:12260\n","step:12270\n","step:12280\n","step:12290\n","step:12300\n","step:12310\n","step:12320\n","step:12330\n","step:12340\n","step:12350\n","step:12360\n","step:12370\n","step:12380\n","step:12390\n","step:12400\n","step:12410\n","step:12420\n","step:12430\n","step:12440\n","step:12450\n","step:12460\n","step:12470\n","step:12480\n","step:12490\n","step:12500\n","step:12510\n","step:12520\n","step:12530\n","step:12540\n","step:12550\n","step:12560\n","step:12570\n","step:12580\n","step:12590\n","step:12600\n","step:12610\n","step:12620\n","step:12630\n","step:12640\n","step:12650\n","step:12660\n","step:12670\n","step:12680\n","step:12690\n","step:12700\n","step:12710\n","step:12720\n","step:12730\n","step:12740\n","step:12750\n","step:12760\n","step:12770\n","step:12780\n","step:12790\n","step:12800\n","step:12810\n","step:12820\n","step:12830\n","step:12840\n","step:12850\n","step:12860\n","step:12870\n","step:12880\n","step:12890\n","step:12900\n","step:12910\n","step:12920\n","step:12930\n","step:12940\n","step:12950\n","step:12960\n","step:12970\n","step:12980\n","step:12990\n","step:13000\n","step:13010\n","step:13020\n","step:13030\n","step:13040\n","step:13050\n","step:13060\n","step:13070\n","step:13080\n","step:13090\n","step:13100\n","step:13110\n","step:13120\n","step:13130\n","step:13140\n","step:13150\n","step:13160\n","step:13170\n","step:13180\n","step:13190\n","step:13200\n","step:13210\n","step:13220\n","step:13230\n","step:13240\n","step:13250\n","step:13260\n","step:13270\n","step:13280\n","step:13290\n","step:13300\n","step:13310\n","step:13320\n","step:13330\n","step:13340\n","step:13350\n","step:13360\n","step:13370\n","step:13380\n","step:13390\n","step:13400\n","step:13410\n","step:13420\n","step:13430\n","step:13440\n","step:13450\n","step:13460\n","step:13470\n","step:13480\n","step:13490\n","step:13500\n","step:13510\n","step:13520\n","step:13530\n","step:13540\n","step:13550\n","step:13560\n","step:13570\n","step:13580\n","step:13590\n","step:13600\n","step:13610\n","step:13620\n","step:13630\n","step:13640\n","step:13650\n","step:13660\n","step:13670\n","step:13680\n","step:13690\n","step:13700\n","step:13710\n","step:13720\n","step:13730\n","step:13740\n","step:13750\n","step:13760\n","step:13770\n","step:13780\n","\n","  Average training loss: 0.0091\n","  Training epoch took: 1:35:03\n","\n","Running Validation...\n"]}],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","\n","\n","# Function to format elapsed time as hh:mm:ss\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# ========================================\n","#               Parameters\n","# ========================================\n","\n","# Training parameters\n","batch_size = 2 # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","checkpoint_interval = 1  # Save checkpoint every N epochs\n","step_interval = 10\n","\n","# Gradient accumulation parameters\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint-5\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint-5\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","\n","import os\n","import re\n","import ast\n","import json\n","import random\n","from cleaner import normalize_text\n","\n","\n","!pip install datasets\n","from datasets import load_dataset\n","\n","# Load the DailyDialog dataset\n","dataset = load_dataset(\"daily_dialog\")\n","\n","# Check the available splits\n","print(dataset)\n","\n","class ChatDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        conversations = []\n","\n","        # First get Hugging Face data\n","        print(f\"loading Hugging Face data\")\n","        conversations.extend(self.get_conversations_HF())\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get Cornell data\n","        print(f\"loading Cornell data\")\n","        data_path = \"data/cornell_movie_dialogs_corpus\"\n","        conversations.extend(self.get_conversations_cornell(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get woz data\n","        print(f\"loading woz data\")\n","        data_path = 'data/MultiWOZ_2.2'\n","        conversations.extend(self.get_conversations_woz(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get taskmaster data\n","        print(f\"loading taskmaster data\")\n","        data_path = 'data/Taskmaster'\n","        conversations.extend(self.get_conversations_taskmaster(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        conversations_list = []\n","        dialogue_list = []\n","        for dialogue in  conversations:\n","            dialogue_list.append(\"<|startoftext|>\")\n","            for line in dialogue:\n","                dialogue_list.append(line + '\\n')\n","            dialogue_list.append('<|endoftext|>')\n","            conversations_list.append(''.join(dialogue_list))\n","            dialogue_list = []\n","        random.shuffle(conversations_list)\n","        text = ''.join(conversations_list)\n","        print(f\"{len(text)=}\")\n","\n","        # Tokenize the text into a sequence of input tokens\n","        tokenized_text = tokenizer.encode(text)\n","\n","        # Split the tokenized text into chunks of max_length\n","        self.input_ids = []\n","        self.attn_masks = []\n","\n","        for i in range(0, len(tokenized_text) - max_length + 1, max_length):\n","            chunk = tokenized_text[i:i + max_length]\n","\n","            # Create padding if needed (only needed if chunks aren't exactly max_length)\n","            padding_length = max_length - len(chunk)\n","\n","            assert padding_length == 0, f\"Padding length should be 0, but got {padding_length}\"\n","\n","            # Pad input IDs with tokenizer's pad token id if necessary\n","            padded_chunk = chunk + [tokenizer.pad_token_id] * padding_length\n","\n","            # Create an attention mask: 1 for tokens, 0 for padding\n","            attention_mask = [1] * len(chunk) + [0] * padding_length\n","\n","            self.input_ids.append(padded_chunk)\n","            self.attn_masks.append(attention_mask)\n","        print(f\"Number of input sequences: {len(self.input_ids)}\")\n","\n","    def get_conversations_HF(self):\n","        train = []\n","        val = []\n","        test = []\n","        the_datasets = {'train': train, 'validation': val, 'test': test}\n","\n","        types = ['train', 'validation', 'test']\n","\n","        pattern = r'\\s([,.!?;:])'\n","        # Regular expressions for spaces around parentheses\n","        pattern_left_parenthesis = r'\\(\\s'\n","        pattern_right_parenthesis = r'\\s\\)'\n","\n","        for t in types:\n","            print(f\"loading: {t=}\")\n","            ds = dataset[t]\n","            # Access specific fields\n","            print(f\"{len(ds)=}\")\n","\n","            for i in range(len(ds)):\n","                dialogue = ds[i]['dialog']\n","                turn = [\"Q: \", \"A: \"]\n","                conv = []\n","                for i, s in enumerate(dialogue):\n","                    s = s.strip()\n","                    s = normalize_text(s)\n","                    s = s.replace(\" ' \", \"'\")\n","                    s = s.replace('$ ', '$')\n","                    s = s.replace('( ', '(')\n","                    s = s.replace(') ', ')')\n","                    s = re.sub(pattern, r'\\1', s)\n","                    s = turn[0 if (i+1)%2 == 1 else 1] + s\n","                    conv.append(s)\n","                the_datasets[t].append(tuple(conv))\n","\n","        return the_datasets['train'] + the_datasets['validation'] + the_datasets['test']\n","\n","    def get_conversations_cornell(self, data_path):\n","        conversations = []\n","\n","        # Load movie lines\n","        id2line = {}\n","        with open(os.path.join(data_path, 'movie_lines.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 5:\n","                    line_id, text = parts[0], parts[4]\n","                    id2line[line_id] = text\n","\n","        # Load conversations\n","        with open(os.path.join(data_path, 'movie_conversations.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 4:\n","                    conv_line_ids = ast.literal_eval(parts[3])  # safer than eval()\n","                    # Create pairs of conversations (input, response)\n","                    for i in range(len(conv_line_ids) - 1):\n","                        # Ensure both line IDs are in id2line\n","                        if conv_line_ids[i] in id2line and conv_line_ids[i + 1] in id2line:\n","                            input_line = \"Q: \" + normalize_text(id2line[conv_line_ids[i]])\n","                            response_line = \"A: \" + normalize_text(id2line[conv_line_ids[i + 1]])\n","                            conversations.append((input_line, response_line))\n","                        #else:\n","                        #    print(f\"Missing line ID in conversation: {conv_line_ids[i]} or {conv_line_ids[i + 1]}\")\n","\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def get_conversations_woz(self, data_path):\n","        turn = ['Q: ', 'A: ']\n","        types = ['train', 'dev', 'test']\n","        the_datasets = {t : [] for t in types}\n","        dir = {t : os.path.join(data_path, t) for t in types}\n","        for t in types:\n","            print(f\"processing: {t=}\")\n","            json_files = [f for f in os.listdir(dir[t]) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            for json_file in json_files:\n","                file_path = os.path.join(dir[t], json_file)\n","                with open(file_path, 'r') as file:\n","                    dialogues.extend(json.load(file))\n","            for dialogue in dialogues:\n","                conversation_list = []\n","                conversation = dialogue['turns']\n","                for i, line in enumerate(conversation):\n","                    conversation_list.append(turn[0 if (i+1)%2 == 1 else 1] + normalize_text(line['utterance']))\n","                the_datasets[t].append(tuple(conversation_list))\n","            print(f\"{len(the_datasets[t])=}\")\n","\n","        return the_datasets['train'] + the_datasets['dev'] + the_datasets['test']\n","\n","    def get_conversations_taskmaster(self, data_path):\n","        dirs = ['TM-1-2019', 'TM-2-2020/data', 'TM-3-2020/data', 'TM-4-2024/data']\n","        conversations = []\n","        for dir in dirs:\n","            print(f\"{dir=}\")\n","            path = os.path.join(data_path, dir)\n","            json_files = [f for f in os.listdir(path) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            print(\"loading JSON files\")\n","            for json_file in json_files:\n","                file_path = os.path.join(path, json_file)\n","                with open(file_path, 'r') as file:\n","                    print(f\"{file_path=}\")\n","                    dialogues.extend(json.load(file))\n","            print(\"processing dialogues\")\n","            for dialogue in dialogues:\n","                utterances = dialogue['utterances']\n","                conversation_list = []\n","                previous_prompt = \"\"\n","                for line in utterances:\n","                    prompt = \"A: \"\n","                    if line['speaker'].lower() == 'user':\n","                        prompt = \"Q: \"\n","                    if previous_prompt == prompt:\n","                        last_text = conversation_list.pop()\n","                        conversation_list.append(last_text + \" \" + normalize_text(line['text']))\n","                    else:\n","                        conversation_list.append(prompt + normalize_text(line['text']))\n","                        previous_prompt = prompt\n","                conversations.append(tuple(conversation_list))\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.input_ids[idx]\n","        attn_mask = self.attn_masks[idx]\n","        return torch.tensor(input_ids), torch.tensor(attn_mask)\n","\n","\n","# Initialize dataset and dataloaders\n","dataset = ChatDataset(tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:,} training samples')\n","print(f'{val_size:,} validation samples')\n","\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n","validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","\n","# ========================================\n","#          Optimizer and Scheduler\n","# ========================================\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n","\n","# Calculate total steps\n","total_steps = len(train_dataloader) * epochs\n","\n","# Prepare learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",")\n","\n","# Initialize mixed precision training scaler with updated API\n","scaler = GradScaler()\n","\n","# ========================================\n","#          Training and Validation\n","# ========================================\n","\n","training_stats = []\n","initial_t0 = time.time()  # Measure total training time\n","\n","# Training loop\n","for epoch_i in range(epochs):\n","    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n","    print('Training...')\n","\n","    t0 = time.time()  # Measure epoch training time\n","    total_train_loss = 0  # Reset the total loss for this epoch\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        model.zero_grad()  # Clear any previously calculated gradients\n","\n","        # Forward pass and compute loss with mixed precision\n","        with autocast(dtype=torch.float16):\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n","\n","        total_train_loss += loss.item()  # Accumulate the loss\n","\n","        scaler.scale(loss).backward()  # Backward pass with scaled loss\n","\n","        if (step + 1) % step_interval == 0:\n","            print(f\"step:{step+1}\")\n","\n","        # Update parameters every `gradient_accumulation_steps`\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            model.zero_grad()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)  # Calculate the average loss\n","    training_time = format_time(time.time() - t0)\n","\n","    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n","    print(f\"  Training epoch took: {training_time}\")\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    print(\"\\nRunning Validation...\")\n","\n","    t0 = time.time()  # Measure validation time\n","    total_eval_loss = 0\n","    model.eval()\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss\n","\n","        total_eval_loss += loss.item()\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Validation took: {validation_time}\")\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Valid. Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","    print(\"\\nEpoch Summary:\")\n","    print(f\"  Epoch {epoch_i + 1} / {epochs}\")\n","    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Training Time: {training_time}\")\n","    print(f\"  Validation Time: {validation_time}\")\n","\n","    # ========================================\n","    #               Sampling\n","    # ========================================\n","    print(\"\\nGenerating Sample Output...\")\n","\n","    model.eval()\n","    sample_outputs = model.generate(\n","        bos_token_id=random.randint(1, 30000),\n","        do_sample=True,\n","        top_k=50,\n","        max_length=200,\n","        top_p=0.95,\n","        num_return_sequences=1\n","    )\n","    for i, sample_output in enumerate(sample_outputs):\n","        print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","    model.train()\n","\n","    # Save model checkpoint after each epoch\n","    if (epoch_i + 1) % checkpoint_interval == 0:\n","        checkpoint_dir = f'./checkpoint_chat-{epoch_i + 1}'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        model.save_pretrained(checkpoint_dir)\n","        tokenizer.save_pretrained(checkpoint_dir)\n","\n","print(\"\\nTraining complete!\")\n","print(f\"Total training took {format_time(time.time() - initial_t0)} (h:mm:ss)\")\n","\n","# ========================================\n","#               Plotting\n","# ========================================\n","epochs = [x['epoch'] for x in training_stats]\n","training_loss = [x['Training Loss'] for x in training_stats]\n","validation_loss = [x['Valid. Loss'] for x in training_stats]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_loss, label='Training Loss')\n","plt.plot(epochs, validation_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.cuda.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint_chat-2\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint_chat-2\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# Prepare input prompt\n","prompt = \"<|startoftext|>Q: To whome am I speaking?\\nA: \"\n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","print(generated)\n","\n","# Generate text\n","sample_outputs = model.generate(\n","                                generated,\n","                                do_sample=True,\n","                                top_k=50,\n","                                max_length=300,\n","                                top_p=0.95,\n","                                num_return_sequences=1\n","                                )\n","\n","# Display generated text\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n","\n","model.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746,"referenced_widgets":["5f3434c2bb0240cf9ea91762695309f2","ac4934df10aa4e5f8bfccbaac52cd2f8","fa5322d7bfbc440fb705ae103045e41d","9da70319fa9a418d976610efb38b7e84","f127d067e4e84be5a4778cdd3b96018b","7ffbd9fe9ed24e6aa43f14904660b686","84bf29515fcb4c8289e490e37de83ffc","ac070a13b7fc49bb8d673d237743faa3","13ecf2e440bf445597a99df0370e2020","3ed92897c12a4e0abb52f017a57caf90","933bba432e424d368042adb1377e002f"]},"id":"PZibPKmsy8kL","executionInfo":{"status":"ok","timestamp":1724845757726,"user_tz":-60,"elapsed":9768,"user":{"displayName":"simon mullen","userId":"04641808595381754594"}},"outputId":"a1431851-e91a-4f35-a038-06ebf95054b9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/gothic\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3434c2bb0240cf9ea91762695309f2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[  27,   91, 9688, 1659, 5239,   91,   29,   48,   25, 1675,  348,  462,\n","          716,  314, 5486,   30,  198,   32,   25,  220]], device='cuda:0')\n","0: <|startoftext|>Q: To whome am I speaking?\n","A:  I have to speak a bit more slowly when I talk, because we need more information.\n","<|endoftext|>\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1600)\n","    (wpe): Embedding(1024, 1600)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-47): 48 x GPT2Block(\n","        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","\n","\n","# Function to format elapsed time as hh:mm:ss\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# ========================================\n","#               Parameters\n","# ========================================\n","\n","# Training parameters\n","batch_size = 2 # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","checkpoint_interval = 1  # Save checkpoint every N epochs\n","step_interval = 100\n","\n","print(f\"batch_size: {batch_size}\")\n","print(f\"max_length: {max_length}\")\n","print(f\"epochs: {epochs}\")\n","print(f\"learning_rate: {learning_rate}\")\n","print(f\"warmup_steps: {warmup_steps}\")\n","print(f\"epsilon: {epsilon}\")\n","\n","# Gradient accumulation parameters\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint_chat-2\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint_chat-2\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Device configuration\n","# Set device_type based on the availability of CUDA\n","device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = torch.device(device_type)\n","model.to(device)\n","print(f\"device_type: {device_type}\")\n","\n","\n","import os\n","import re\n","import ast\n","import json\n","import random\n","from cleaner import normalize_text\n","\n","\n","!pip install datasets\n","from datasets import load_dataset\n","\n","# Load the DailyDialog dataset\n","dataset = load_dataset(\"daily_dialog\")\n","\n","# Check the available splits\n","print(dataset)\n","\n","class ChatDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        conversations = []\n","\n","        # First get Hugging Face data\n","        print(f\"loading Hugging Face data\")\n","        conversations.extend(self.get_conversations_HF())\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get Cornell data\n","        print(f\"loading Cornell data\")\n","        data_path = \"data/cornell_movie_dialogs_corpus\"\n","        conversations.extend(self.get_conversations_cornell(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get woz data\n","        print(f\"loading woz data\")\n","        data_path = 'data/MultiWOZ_2.2'\n","        conversations.extend(self.get_conversations_woz(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get taskmaster data\n","        print(f\"loading taskmaster data\")\n","        data_path = 'data/Taskmaster'\n","        conversations.extend(self.get_conversations_taskmaster(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        conversations_list = []\n","        dialogue_list = []\n","        for dialogue in  conversations:\n","            dialogue_list.append(\"<|startoftext|>\")\n","            for line in dialogue:\n","                dialogue_list.append(line + '\\n')\n","            dialogue_list.append('<|endoftext|>')\n","            conversations_list.append(''.join(dialogue_list))\n","            dialogue_list = []\n","        random.shuffle(conversations_list)\n","        text = ''.join(conversations_list)\n","        print(f\"{len(text)=}\")\n","\n","        # Tokenize the text into a sequence of input tokens\n","        tokenized_text = tokenizer.encode(text)\n","        print(f\"{len(tokenized_text)=}\")\n","\n","        # Split the tokenized text into chunks of max_length\n","        self.input_ids = []\n","        self.attn_masks = []\n","\n","        for i in range(0, len(tokenized_text) - max_length + 1, max_length):\n","            chunk = tokenized_text[i:i + max_length]\n","\n","            # Create padding if needed (only needed if chunks aren't exactly max_length)\n","            padding_length = max_length - len(chunk)\n","\n","            assert padding_length == 0, f\"Padding length should be 0, but got {padding_length}\"\n","\n","            # Pad input IDs with tokenizer's pad token id if necessary\n","            padded_chunk = chunk + [tokenizer.pad_token_id] * padding_length\n","\n","            # Create an attention mask: 1 for tokens, 0 for padding\n","            attention_mask = [1] * len(chunk) + [0] * padding_length\n","\n","            self.input_ids.append(padded_chunk)\n","            self.attn_masks.append(attention_mask)\n","        print(f\"Number of input sequences: {len(self.input_ids)}\")\n","\n","    def get_conversations_HF(self):\n","        train = []\n","        val = []\n","        test = []\n","        the_datasets = {'train': train, 'validation': val, 'test': test}\n","\n","        types = ['train', 'validation', 'test']\n","\n","        pattern = r'\\s([,.!?;:])'\n","        # Regular expressions for spaces around parentheses\n","        pattern_left_parenthesis = r'\\(\\s'\n","        pattern_right_parenthesis = r'\\s\\)'\n","\n","        for t in types:\n","            print(f\"loading: {t=}\")\n","            ds = dataset[t]\n","            # Access specific fields\n","            print(f\"{len(ds)=}\")\n","\n","            for i in range(len(ds)):\n","                dialogue = ds[i]['dialog']\n","                turn = [\"Q: \", \"A: \"]\n","                conv = []\n","                for i, s in enumerate(dialogue):\n","                    s = s.strip()\n","                    s = normalize_text(s)\n","                    s = s.replace(\" ' \", \"'\")\n","                    s = s.replace('$ ', '$')\n","                    s = s.replace('( ', '(')\n","                    s = s.replace(') ', ')')\n","                    s = re.sub(pattern, r'\\1', s)\n","                    s = turn[0 if (i+1)%2 == 1 else 1] + s\n","                    conv.append(s)\n","                the_datasets[t].append(tuple(conv))\n","\n","        return the_datasets['train'] + the_datasets['validation'] + the_datasets['test']\n","\n","    def get_conversations_cornell(self, data_path):\n","        conversations = []\n","\n","        # Load movie lines\n","        id2line = {}\n","        with open(os.path.join(data_path, 'movie_lines.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 5:\n","                    line_id, text = parts[0], parts[4]\n","                    id2line[line_id] = text\n","\n","        # Load conversations\n","        with open(os.path.join(data_path, 'movie_conversations.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 4:\n","                    conv_line_ids = ast.literal_eval(parts[3])  # safer than eval()\n","                    # Create pairs of conversations (input, response)\n","                    for i in range(len(conv_line_ids) - 1):\n","                        # Ensure both line IDs are in id2line\n","                        if conv_line_ids[i] in id2line and conv_line_ids[i + 1] in id2line:\n","                            input_line = \"Q: \" + normalize_text(id2line[conv_line_ids[i]])\n","                            response_line = \"A: \" + normalize_text(id2line[conv_line_ids[i + 1]])\n","                            conversations.append((input_line, response_line))\n","                        #else:\n","                        #    print(f\"Missing line ID in conversation: {conv_line_ids[i]} or {conv_line_ids[i + 1]}\")\n","\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def get_conversations_woz(self, data_path):\n","        turn = ['Q: ', 'A: ']\n","        types = ['train', 'dev', 'test']\n","        the_datasets = {t : [] for t in types}\n","        dir = {t : os.path.join(data_path, t) for t in types}\n","        for t in types:\n","            print(f\"processing: {t=}\")\n","            json_files = [f for f in os.listdir(dir[t]) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            for json_file in json_files:\n","                file_path = os.path.join(dir[t], json_file)\n","                with open(file_path, 'r') as file:\n","                    dialogues.extend(json.load(file))\n","            for dialogue in dialogues:\n","                conversation_list = []\n","                conversation = dialogue['turns']\n","                for i, line in enumerate(conversation):\n","                    conversation_list.append(turn[0 if (i+1)%2 == 1 else 1] + normalize_text(line['utterance']))\n","                the_datasets[t].append(tuple(conversation_list))\n","            print(f\"{len(the_datasets[t])=}\")\n","\n","        return the_datasets['train'] + the_datasets['dev'] + the_datasets['test']\n","\n","    def get_conversations_taskmaster(self, data_path):\n","        dirs = ['TM-1-2019', 'TM-2-2020/data', 'TM-3-2020/data', 'TM-4-2024/data']\n","        conversations = []\n","        for dir in dirs:\n","            print(f\"{dir=}\")\n","            path = os.path.join(data_path, dir)\n","            json_files = [f for f in os.listdir(path) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            print(\"loading JSON files\")\n","            for json_file in json_files:\n","                file_path = os.path.join(path, json_file)\n","                with open(file_path, 'r') as file:\n","                    print(f\"{file_path=}\")\n","                    dialogues.extend(json.load(file))\n","            print(\"processing dialogues\")\n","            for dialogue in dialogues:\n","                utterances = dialogue['utterances']\n","                conversation_list = []\n","                previous_prompt = \"\"\n","                for line in utterances:\n","                    prompt = \"A: \"\n","                    if line['speaker'].lower() == 'user':\n","                        prompt = \"Q: \"\n","                    if previous_prompt == prompt:\n","                        last_text = conversation_list.pop()\n","                        conversation_list.append(last_text + \" \" + normalize_text(line['text']))\n","                    else:\n","                        conversation_list.append(prompt + normalize_text(line['text']))\n","                        previous_prompt = prompt\n","                conversations.append(tuple(conversation_list))\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.input_ids[idx]\n","        attn_mask = self.attn_masks[idx]\n","        return torch.tensor(input_ids), torch.tensor(attn_mask)\n","\n","\n","# Initialize dataset and dataloaders\n","dataset = ChatDataset(tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:,} training samples')\n","print(f'{val_size:,} validation samples')\n","\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n","validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","\n","# ========================================\n","#          Optimizer and Scheduler\n","# ========================================\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n","\n","# Calculate total steps\n","total_steps = len(train_dataloader) * epochs\n","\n","# Prepare learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",")\n","\n","# Initialize mixed precision training scaler with updated API\n","scaler = GradScaler()\n","\n","# ========================================\n","#          Training and Validation\n","# ========================================\n","\n","training_stats = []\n","initial_t0 = time.time()  # Measure total training time\n","\n","# Training loop\n","for epoch_i in range(epochs):\n","    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n","    print('Training...')\n","\n","    t0 = time.time()  # Measure epoch training time\n","    total_train_loss = 0  # Reset the total loss for this epoch\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        model.zero_grad()  # Clear any previously calculated gradients\n","\n","        # Forward pass and compute loss with mixed precision\n","        with autocast(device_type = 'cuda', dtype=torch.float16):\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n","\n","        total_train_loss += loss.item()  # Accumulate the loss\n","\n","        scaler.scale(loss).backward()  # Backward pass with scaled loss\n","\n","        if (step + 1) % step_interval == 0:\n","            print(f\"step:{step+1}\")\n","\n","        # Update parameters every `gradient_accumulation_steps`\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            model.zero_grad()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)  # Calculate the average loss\n","    training_time = format_time(time.time() - t0)\n","\n","    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n","    print(f\"  Training epoch took: {training_time}\")\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    print(\"\\nRunning Validation...\")\n","\n","    t0 = time.time()  # Measure validation time\n","    total_eval_loss = 0\n","    model.eval()\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss\n","\n","        total_eval_loss += loss.item()\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Validation took: {validation_time}\")\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Valid. Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","    print(\"\\nEpoch Summary:\")\n","    print(f\"  Epoch {epoch_i + 1} / {epochs}\")\n","    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Training Time: {training_time}\")\n","    print(f\"  Validation Time: {validation_time}\")\n","\n","    # ========================================\n","    #               Sampling\n","    # ========================================\n","    print(\"\\nGenerating Sample Output...\")\n","\n","    model.eval()\n","    sample_outputs = model.generate(\n","        bos_token_id=random.randint(1, 30000),\n","        do_sample=True,\n","        top_k=50,\n","        max_length=200,\n","        top_p=0.95,\n","        num_return_sequences=1\n","    )\n","    for i, sample_output in enumerate(sample_outputs):\n","        print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","    model.train()\n","\n","    # Save model checkpoint after each epoch\n","    if (epoch_i + 1) % checkpoint_interval == 0:\n","        checkpoint_dir = f'./checkpoint_chat_2-{epoch_i + 1}'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        model.save_pretrained(checkpoint_dir)\n","        tokenizer.save_pretrained(checkpoint_dir)\n","\n","print(\"\\nTraining complete!\")\n","print(f\"Total training took {format_time(time.time() - initial_t0)} (h:mm:ss)\")\n","\n","# ========================================\n","#               Plotting\n","# ========================================\n","epochs = [x['epoch'] for x in training_stats]\n","training_loss = [x['Training Loss'] for x in training_stats]\n","validation_loss = [x['Valid. Loss'] for x in training_stats]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_loss, label='Training Loss')\n","plt.plot(epochs, validation_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["acbe72fb5549400abe63ffe69e4af898","46c28ab6d5914bb3bed138e950fc7604","80efed9300ae49eda8c115be1ec14939","8c6e88220ec146b29641527806b35bf3","6d598c3eab0d47968f603c5a2e35ed5c","33b5e920621b40db894124a6876221b1","b1485166d1bb4dfc8e9ee700c2b70480","2d150e89b21447f2b51f27b30afbdf31","38aeb2d651bc44bdb19b51db1a973cf3","5df8de62e8dc4a8081018c35ec5ae50a","ddd77c6464fb42e3b5670e1108ca192c"]},"id":"DNgtmdwy1xAh","outputId":"d09bb7d5-ca99-46ee-da41-564de1986f01"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["batch_size: 2\n","max_length: 1024\n","epochs: 5\n","learning_rate: 0.0005\n","warmup_steps: 100.0\n","epsilon: 1e-08\n","Gradient accumulation steps: 256\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab Notebooks/gothic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acbe72fb5549400abe63ffe69e4af898","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["device_type: cuda\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 11118\n","    })\n","    validation: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['dialog', 'act', 'emotion'],\n","        num_rows: 1000\n","    })\n","})\n","loading Hugging Face data\n","loading: t='train'\n","len(ds)=11118\n","loading: t='validation'\n","len(ds)=1000\n","loading: t='test'\n","len(ds)=1000\n","len(conversations)=13118\n","loading Cornell data\n","len(conversations)=221282\n","len(conversations)=234400\n","loading woz data\n","processing: t='train'\n","len(the_datasets[t])=8437\n","processing: t='dev'\n","len(the_datasets[t])=1000\n","processing: t='test'\n","len(the_datasets[t])=1000\n","len(conversations)=244837\n","loading taskmaster data\n","dir='TM-1-2019'\n","loading JSON files\n","file_path='data/Taskmaster/TM-1-2019/woz-dialogs.json'\n","file_path='data/Taskmaster/TM-1-2019/self-dialogs.json'\n","processing dialogues\n","dir='TM-2-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-2-2020/data/flights.json'\n","file_path='data/Taskmaster/TM-2-2020/data/music.json'\n","file_path='data/Taskmaster/TM-2-2020/data/hotels.json'\n","file_path='data/Taskmaster/TM-2-2020/data/restaurant-search.json'\n","file_path='data/Taskmaster/TM-2-2020/data/sports.json'\n","file_path='data/Taskmaster/TM-2-2020/data/movies.json'\n","file_path='data/Taskmaster/TM-2-2020/data/food-ordering.json'\n","processing dialogues\n","dir='TM-3-2020/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-3-2020/data/data_15.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_05.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_18.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_09.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_16.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_02.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_14.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_07.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_00.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_04.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_03.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_01.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_11.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_12.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_10.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_19.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_13.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_17.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_08.json'\n","file_path='data/Taskmaster/TM-3-2020/data/data_06.json'\n","processing dialogues\n","dir='TM-4-2024/data'\n","loading JSON files\n","file_path='data/Taskmaster/TM-4-2024/data/data_04.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_01.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_06.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_00.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_02.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_03.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_05.json'\n","file_path='data/Taskmaster/TM-4-2024/data/data_07.json'\n","processing dialogues\n","len(conversations)=57986\n","len(conversations)=302823\n","len(text)=108960154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (31377861 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["len(tokenized_text)=31377861\n","Number of input sequences: 30642\n","27,577 training samples\n","3,065 validation samples\n","\n","======== Epoch 1 / 5 ========\n","Training...\n","step:100\n","step:200\n","step:300\n","step:400\n","step:500\n","step:600\n","step:700\n","step:800\n","step:900\n","step:1000\n","step:1100\n","step:1200\n","step:1300\n","step:1400\n","step:1500\n","step:1600\n","step:1700\n","step:1800\n","step:1900\n","step:2000\n","step:2100\n","step:2200\n","step:2300\n","step:2400\n","step:2500\n","step:2600\n","step:2700\n","step:2800\n","step:2900\n","step:3000\n","step:3100\n","step:3200\n","step:3300\n","step:3400\n","step:3500\n","step:3600\n","step:3700\n","step:3800\n","step:3900\n","step:4000\n","step:4100\n","step:4200\n","step:4300\n","step:4400\n","step:4500\n","step:4600\n","step:4700\n","step:4800\n","step:4900\n","step:5000\n","step:5100\n","step:5200\n","step:5300\n","step:5400\n","step:5500\n","step:5600\n","step:5700\n","step:5800\n","step:5900\n","step:6000\n","step:6100\n","step:6200\n","step:6300\n","step:6400\n","step:6500\n","step:6600\n","step:6700\n","step:6800\n","step:6900\n","step:7000\n","step:7100\n","step:7200\n","step:7300\n","step:7400\n","step:7500\n","step:7600\n","step:7700\n","step:7800\n","step:7900\n","step:8000\n","step:8100\n","step:8200\n","step:8300\n","step:8400\n","step:8500\n","step:8600\n","step:8700\n","step:8800\n","step:8900\n","step:9000\n","step:9100\n","step:9200\n","step:9300\n","step:9400\n","step:9500\n","step:9600\n","step:9700\n","step:9800\n","step:9900\n","step:10000\n","step:10100\n","step:10200\n","step:10300\n","step:10400\n","step:10500\n","step:10600\n","step:10700\n","step:10800\n","step:10900\n","step:11000\n","step:11100\n","step:11200\n","step:11300\n","step:11400\n","step:11500\n","step:11600\n","step:11700\n","step:11800\n","step:11900\n","step:12000\n","step:12100\n","step:12200\n","step:12300\n","step:12400\n","step:12500\n","step:12600\n","step:12700\n","step:12800\n","step:12900\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkbnkXmqrZCC"},"outputs":[],"source":["model.eval()\n","\n","# Prepare input prompt\n","prompt = \"<|startoftext|>Q: To whome am I speaking?\\nA: \"\n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","print(generated)\n","\n","# Generate text\n","sample_outputs = model.generate(\n","                                generated,\n","                                do_sample=True,\n","                                top_k=50,\n","                                max_length=300,\n","                                top_p=0.95,\n","                                num_return_sequences=1\n","                                )\n","\n","# Display generated text\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPJoNeKqJTJN","outputId":"ccbab5fb-d17a-46ac-f3d5-f6b3cebcef76"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>4.789589</td>\n","      <td>1.894683</td>\n","      <td>1:50:03</td>\n","      <td>0:02:39</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.871892</td>\n","      <td>1.804658</td>\n","      <td>1:46:04</td>\n","      <td>0:02:53</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.786069</td>\n","      <td>1.785056</td>\n","      <td>1:37:31</td>\n","      <td>0:02:40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss Training Time Validation Time\n","epoch                                                          \n","1           4.789589     1.894683       1:50:03         0:02:39\n","2           1.871892     1.804658       1:46:04         0:02:53\n","3           1.786069     1.785056       1:37:31         0:02:40"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"h3ErOZ2MJTJN"},"source":["## Saving & Loading Fine-Tuned Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkc3PGkhJTJN","outputId":"ff5af8fc-8b3f-437f-eb6a-d94cb58ad7ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./model\n"]},{"data":{"text/plain":["('./model\\\\tokenizer_config.json',\n"," './model\\\\special_tokens_map.json',\n"," './model\\\\vocab.json',\n"," './model\\\\merges.txt',\n"," './model\\\\added_tokens.json',\n"," './model\\\\tokenizer.json')"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Saving model to %s\" % model_save_path)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model.save_pretrained(model_save_path)\n","tokenizer.save_pretrained(model_save_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6OS6AOvJTJO"},"outputs":[],"source":["model = GPT2LMHeadModel.from_pretrained(model_save_path)\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cnVODli8JTJO"},"source":["## Generate Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4nssaSVJTJM"},"outputs":[],"source":["def infer(prompt):\n","    input = f\"<|startoftext|>Ingredients: {prompt.strip()}\"\n","    input = tokenizer(input, return_tensors=\"pt\")\n","    input_ids      = input[\"input_ids\"]\n","    attention_mask = input[\"attention_mask\"]\n","\n","    output = model.generate(input_ids.to(device),\n","                            attention_mask=attention_mask.to(device),\n","                            max_new_tokens=max_length,\n","                            # temperature = 0.5,\n","                            do_sample = True, top_k = 50, top_p = 0.85)\n","                            # num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n","    output = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhHaArMVJTJO","outputId":"b5e1731a-e5da-4fcd-a524-dde594dd5f3b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Ingredients: eggs, mushroom, butter, sugar, milk, cream, cornstarch, salt, brown sugar, flour, baking powder, baking soda, baking soda. Instructions: In a large saucepan, bring eggs to boil. Reduce heat and simmer until thickened. Remove from heat and stir in mushroom, butter, sugar, milk, cornstarch, salt, and brown sugar. Add the flour, baking powder and baking soda; stir until well blended. Add the baking soda and stir until completely dissolved. Add milk and cornstarch mixture to the mushroom mixture.  Stir well to combine. Divide mixture into 8 equal parts and bake at 350 degrees for 15 minutes. Let stand 5 minutes before serving. Makes 4-6 servings.\n"]}],"source":["# model = GPT2LMHeadModel.from_pretrained(model_save_path)\n","# tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n","# model.to(device)\n","print(infer(\"eggs, mushroom, butter, sugar\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dT4kmGYOJTJP","outputId":"5c20deb9-5553-46bc-b8f3-8ce0527defe9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/plain":["'Ingredients: onion, garlic, chicken breast, fresh basil, butter, butter, brown sugar, salt. Instructions: Place chicken breasts in a slow cooker on low heat. Cook for 3 hours. Drain well. Pour 1/2 teaspoon of oil into a mixing bowl. Add onion and garlic. Stir and cook for another 3 hours or until translucent. Add chicken breast. Add fresh basil, butter and stir until well blended. Pour mixture into slow cooker. Cover and cook for 1 hour or until tender.'"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["infer(\"onion, garlic, chicken breast\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAbGhYs7JTJP","outputId":"a3e903cd-3bef-4e69-8bed-3ff8c54efcae"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Ingredients: avocado, lime juice, lime zest, lime juice, soy sauce, lime zest, soy sauce, lemon juice, fish sauce, lime, coconut, brown sugar, brown sugar, brown sugar, fish sauce, coconut. Instructions: Mix together avocado, lime juice, zest, lime juice, soy sauce, lime z zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime zest, lime juice, soy, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime juice, soy sauce, lime juice, soy sauce, lime zest, lime\n"]}],"source":["print(infer(\"avocado, lime\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50P-5xLHJTJQ"},"outputs":[],"source":["# Using BLEU score to compare the real sentences with the generated ones\n","import statistics\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","scores=[]\n","\n","for i in range(10):\n","    ingredients = val_dataset[i][2]\n","    reference = val_dataset[i][3]\n","    candidate = infer(ingredients)\n","    scores.append(sentence_bleu(reference, candidate))\n","\n","print(statistics.mean(scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYgAAnE2JTJQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"widgets":{"application/vnd.jupyter.widget-state+json":{"089d94324ade4d9283acc380aa89bf3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dcc0f9b865d4010802ec2fd0ac15aa0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c02701c014f4f35b15070ac7a8cc409","value":2}},"0e2b14d92cdb4040bc82f3e021feccfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_947c25188a7847fa8c9d0f91094f379f","placeholder":"","style":"IPY_MODEL_62f0057f699d4b9cb93218f521abeb00","value":"2/2[00:03&lt;00:00,1.76s/it]"}},"107c958794a34da9918baa24469d8bd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6df6439d999a43519f8c5ccd6a2b6ac4","IPY_MODEL_ff3d46d6b87b4290acd8428c36b985e9","IPY_MODEL_bc07fe803dc24460ab0fd9870ecaf841"],"layout":"IPY_MODEL_69e3ed9dc023401b8c3866d78ef8dd01"}},"13ee64c77de5467486bb02d76cc9a805":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d28f8a3c35946cf82e31bdbc6e3d4c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1df7177676f4284a42649c377c077cc","IPY_MODEL_b96ffdd3cd0c475f8f9ef4faf364c040","IPY_MODEL_0e2b14d92cdb4040bc82f3e021feccfb"],"layout":"IPY_MODEL_762171df28a7413bb33fd3c3a4c8d5fe"}},"1e0be32905c646bd8828ac56ca2587cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b520f008d4487e97090b6979e48eba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3300727409204cd296f7f2962158357f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b0a3aaad20e4bb6b547a3d3b7fd40ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9fbafdda379439289b6ad58b0292b33","IPY_MODEL_089d94324ade4d9283acc380aa89bf3f","IPY_MODEL_7ef83bbf564141139c6ded647843fcf9"],"layout":"IPY_MODEL_dc4ea86d8e474dd1a5f1c55df6523afb"}},"4c6e6c5a87d04c5db47b01f10537f6fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62f0057f699d4b9cb93218f521abeb00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69e3ed9dc023401b8c3866d78ef8dd01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df6439d999a43519f8c5ccd6a2b6ac4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_810d8ea78b28490882e47140b56574ea","placeholder":"","style":"IPY_MODEL_3300727409204cd296f7f2962158357f","value":"Loadingcheckpointshards:100%"}},"762171df28a7413bb33fd3c3a4c8d5fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dcc0f9b865d4010802ec2fd0ac15aa0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef83bbf564141139c6ded647843fcf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c6e6c5a87d04c5db47b01f10537f6fc","placeholder":"","style":"IPY_MODEL_affd57ec28064be99ea561909496c609","value":"2/2[00:03&lt;00:00,1.80s/it]"}},"810d8ea78b28490882e47140b56574ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8534d1bbbba3402c9a08250eac979a9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"947c25188a7847fa8c9d0f91094f379f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c02701c014f4f35b15070ac7a8cc409":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a925b5e1df5244658201e7c1b78a645c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"affd57ec28064be99ea561909496c609":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b08cf6f31c5a402cad164ba01a0e02aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b48b80da592b4f75abe36eb3d5c866b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b96ffdd3cd0c475f8f9ef4faf364c040":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a925b5e1df5244658201e7c1b78a645c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b48b80da592b4f75abe36eb3d5c866b5","value":2}},"bc07fe803dc24460ab0fd9870ecaf841":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13ee64c77de5467486bb02d76cc9a805","placeholder":"","style":"IPY_MODEL_b08cf6f31c5a402cad164ba01a0e02aa","value":"2/2[00:04&lt;00:00,1.86s/it]"}},"c5ce02efd8854466866d56bd0841317e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1df7177676f4284a42649c377c077cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0be32905c646bd8828ac56ca2587cf","placeholder":"","style":"IPY_MODEL_c5ce02efd8854466866d56bd0841317e","value":"Loadingcheckpointshards:100%"}},"dc4ea86d8e474dd1a5f1c55df6523afb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de7e56c734ee4a198120d164add69928":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9fbafdda379439289b6ad58b0292b33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23b520f008d4487e97090b6979e48eba","placeholder":"","style":"IPY_MODEL_f317b7e6b74d43a585977e1ed5c1d954","value":"Loadingcheckpointshards:100%"}},"f317b7e6b74d43a585977e1ed5c1d954":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff3d46d6b87b4290acd8428c36b985e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de7e56c734ee4a198120d164add69928","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8534d1bbbba3402c9a08250eac979a9b","value":2}},"5f3434c2bb0240cf9ea91762695309f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac4934df10aa4e5f8bfccbaac52cd2f8","IPY_MODEL_fa5322d7bfbc440fb705ae103045e41d","IPY_MODEL_9da70319fa9a418d976610efb38b7e84"],"layout":"IPY_MODEL_f127d067e4e84be5a4778cdd3b96018b"}},"ac4934df10aa4e5f8bfccbaac52cd2f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ffbd9fe9ed24e6aa43f14904660b686","placeholder":"","style":"IPY_MODEL_84bf29515fcb4c8289e490e37de83ffc","value":"Loadingcheckpointshards:100%"}},"fa5322d7bfbc440fb705ae103045e41d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac070a13b7fc49bb8d673d237743faa3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13ecf2e440bf445597a99df0370e2020","value":2}},"9da70319fa9a418d976610efb38b7e84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ed92897c12a4e0abb52f017a57caf90","placeholder":"","style":"IPY_MODEL_933bba432e424d368042adb1377e002f","value":"2/2[00:03&lt;00:00,1.68s/it]"}},"f127d067e4e84be5a4778cdd3b96018b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ffbd9fe9ed24e6aa43f14904660b686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84bf29515fcb4c8289e490e37de83ffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac070a13b7fc49bb8d673d237743faa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13ecf2e440bf445597a99df0370e2020":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ed92897c12a4e0abb52f017a57caf90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"933bba432e424d368042adb1377e002f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acbe72fb5549400abe63ffe69e4af898":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46c28ab6d5914bb3bed138e950fc7604","IPY_MODEL_80efed9300ae49eda8c115be1ec14939","IPY_MODEL_8c6e88220ec146b29641527806b35bf3"],"layout":"IPY_MODEL_6d598c3eab0d47968f603c5a2e35ed5c"}},"46c28ab6d5914bb3bed138e950fc7604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33b5e920621b40db894124a6876221b1","placeholder":"","style":"IPY_MODEL_b1485166d1bb4dfc8e9ee700c2b70480","value":"Loadingcheckpointshards:100%"}},"80efed9300ae49eda8c115be1ec14939":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d150e89b21447f2b51f27b30afbdf31","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38aeb2d651bc44bdb19b51db1a973cf3","value":2}},"8c6e88220ec146b29641527806b35bf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5df8de62e8dc4a8081018c35ec5ae50a","placeholder":"","style":"IPY_MODEL_ddd77c6464fb42e3b5670e1108ca192c","value":"2/2[00:03&lt;00:00,1.76s/it]"}},"6d598c3eab0d47968f603c5a2e35ed5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33b5e920621b40db894124a6876221b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1485166d1bb4dfc8e9ee700c2b70480":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d150e89b21447f2b51f27b30afbdf31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38aeb2d651bc44bdb19b51db1a973cf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5df8de62e8dc4a8081018c35ec5ae50a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd77c6464fb42e3b5670e1108ca192c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
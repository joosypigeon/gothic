{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fXQBraz6JTJK","jupyter":{"outputs_hidden":false}},"source":["## FINE TUNE ON GOTHIC TEXT ##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMUe274RJTJM"},"outputs":[],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Function to format elapsed time as hh:mm:ss\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# ========================================\n","#               Parameters\n","# ========================================\n","\n","# Training parameters\n","batch_size = 2 # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","checkpoint_interval = 1  # Save checkpoint every N epochs\n","step_interval = 10\n","\n","# Gradient accumulation parameters\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ========================================\n","#        Model and Tokenizer Setup\n","# ========================================\n","\n","model_name = \"gpt2-xl\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n","tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","model = model.to(device)\n","\n","# ========================================\n","#               Dataset\n","# ========================================\n","\n","class GothicDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        # Load the text\n","        with open('./gothic_novels_combined.txt', 'r') as f:\n","            text = f.read()\n","        print(f\"Loaded {len(text)} characters\")\n","\n","        # Tokenize the text into a sequence of input tokens\n","        tokenized_text = tokenizer.encode(text)\n","\n","        # Split the tokenized text into chunks of max_length\n","        self.input_ids = []\n","        self.attn_masks = []\n","\n","        for i in range(0, len(tokenized_text) - max_length + 1, max_length):\n","            chunk = tokenized_text[i:i + max_length]\n","\n","            # Create padding if needed (only needed if chunks aren't exactly max_length)\n","            padding_length = max_length - len(chunk)\n","\n","            assert padding_length == 0, f\"Padding length should be 0, but got {padding_length}\"\n","\n","            # Pad input IDs with tokenizer's pad token id if necessary\n","            padded_chunk = chunk + [tokenizer.pad_token_id] * padding_length\n","\n","            # Create an attention mask: 1 for tokens, 0 for padding\n","            attention_mask = [1] * len(chunk) + [0] * padding_length\n","\n","            self.input_ids.append(padded_chunk)\n","            self.attn_masks.append(attention_mask)\n","        print(f\"Number of input sequences: {len(self.input_ids)}\")\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.input_ids[idx]\n","        attn_mask = self.attn_masks[idx]\n","        return torch.tensor(input_ids), torch.tensor(attn_mask)\n","\n","\n","\n","# Initialize dataset and dataloaders\n","dataset = GothicDataset(tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:,} training samples')\n","print(f'{val_size:,} validation samples')\n","\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n","validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","\n","# ========================================\n","#          Optimizer and Scheduler\n","# ========================================\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n","\n","# Calculate total steps\n","total_steps = len(train_dataloader) * epochs\n","\n","# Prepare learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",")\n","\n","# Initialize mixed precision training scaler with updated API\n","scaler = GradScaler()\n","\n","# ========================================\n","#          Training and Validation\n","# ========================================\n","\n","training_stats = []\n","initial_t0 = time.time()  # Measure total training time\n","\n","# Training loop\n","for epoch_i in range(epochs):\n","    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n","    print('Training...')\n","\n","    t0 = time.time()  # Measure epoch training time\n","    total_train_loss = 0  # Reset the total loss for this epoch\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        model.zero_grad()  # Clear any previously calculated gradients\n","\n","        # Forward pass and compute loss with mixed precision\n","        with autocast(device_type='cuda', dtype=torch.float16):\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n","\n","        total_train_loss += loss.item()  # Accumulate the loss\n","\n","        scaler.scale(loss).backward()  # Backward pass with scaled loss\n","\n","        if (step + 1) % step_interval == 0:\n","            print(f\"step:{step+1}\")\n","\n","        # Update parameters every `gradient_accumulation_steps`\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            model.zero_grad()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)  # Calculate the average loss\n","    training_time = format_time(time.time() - t0)\n","\n","    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n","    print(f\"  Training epoch took: {training_time}\")\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    print(\"\\nRunning Validation...\")\n","\n","    t0 = time.time()  # Measure validation time\n","    total_eval_loss = 0\n","    model.eval()\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss\n","\n","        total_eval_loss += loss.item()\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Validation took: {validation_time}\")\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Valid. Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","    print(\"\\nEpoch Summary:\")\n","    print(f\"  Epoch {epoch_i + 1} / {epochs}\")\n","    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Training Time: {training_time}\")\n","    print(f\"  Validation Time: {validation_time}\")\n","\n","    # ========================================\n","    #               Sampling\n","    # ========================================\n","    print(\"\\nGenerating Sample Output...\")\n","\n","    model.eval()\n","    sample_outputs = model.generate(\n","        bos_token_id=random.randint(1, 30000),\n","        do_sample=True,\n","        top_k=50,\n","        max_length=200,\n","        top_p=0.95,\n","        num_return_sequences=1\n","    )\n","    for i, sample_output in enumerate(sample_outputs):\n","        print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","    model.train()\n","\n","    # Save model checkpoint after each epoch\n","    if (epoch_i + 1) % checkpoint_interval == 0:\n","        checkpoint_dir = f'./checkpoint-{epoch_i + 1}'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        model.save_pretrained(checkpoint_dir)\n","        tokenizer.save_pretrained(checkpoint_dir)\n","\n","print(\"\\nTraining complete!\")\n","print(f\"Total training took {format_time(time.time() - initial_t0)} (h:mm:ss)\")\n","\n","# ========================================\n","#               Plotting\n","# ========================================\n","epochs = [x['epoch'] for x in training_stats]\n","training_loss = [x['Training Loss'] for x in training_stats]\n","validation_loss = [x['Valid. Loss'] for x in training_stats]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_loss, label='Training Loss')\n","plt.plot(epochs, validation_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SMcOf3tOtAsA"},"source":["## FINE TUNE ON QUESTION AND ANSWER ##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNgtmdwy1xAh"},"outputs":[],"source":["import os\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","import re\n","import ast\n","import json\n","from cleaner import normalize_text\n","\n","\n","# Function to format elapsed time as hh:mm:ss\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n","\n","# ========================================\n","#               Parameters\n","# ========================================\n","\n","# Training parameters\n","batch_size = 2 # Mini-batch size\n","max_length = 1024  # Maximum sequence length\n","epochs = 5\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","checkpoint_interval = 1  # Save checkpoint every N epochs\n","step_interval = 100\n","\n","print(f\"batch_size: {batch_size}\")\n","print(f\"max_length: {max_length}\")\n","print(f\"epochs: {epochs}\")\n","print(f\"learning_rate: {learning_rate}\")\n","print(f\"warmup_steps: {warmup_steps}\")\n","print(f\"epsilon: {epsilon}\")\n","\n","# Gradient accumulation parameters\n","desired_tokens_per_batch = 524288  # Desired effective batch size in tokens\n","tokens_per_mini_batch = batch_size * max_length\n","gradient_accumulation_steps = desired_tokens_per_batch // tokens_per_mini_batch  # Number of steps to accumulate gradients\n","\n","print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n","\n","\n","\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","# model = GPT2LMHeadModel.from_pretrained(\"./checkpoint_chat-2\")\n","# tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint-5\")\n","# Define your additional special tokens\n","model_name = \"gpt2-xl\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","special_tokens = {\n","    'pad_token': '<|pad|>',\n","    'bos_token': '<|startoftext|>',\n","    'eos_token': '<|endoftext|>',\n","    'additional_special_tokens': ['<|user|>', '<|assistant|>']\n","}\n","tokenizer.add_special_tokens(special_tokens)\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Device configuration\n","# Set device_type based on the availability of CUDA\n","device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = torch.device(device_type)\n","model.to(device)\n","print(f\"device_type: {device_type}\")\n","\n","!pip install datasets\n","from datasets import load_dataset\n","\n","# Load the DailyDialog dataset\n","dataset = load_dataset(\"daily_dialog\")\n","\n","# Check the available splits\n","print(dataset)\n","\n","class ChatDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        conversations = []\n","\n","        # First get Hugging Face data\n","        print(f\"loading Hugging Face data\")\n","        conversations.extend(self.get_conversations_HF())\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get Cornell data\n","        print(f\"loading Cornell data\")\n","        data_path = \"data/cornell_movie_dialogs_corpus\"\n","        conversations.extend(self.get_conversations_cornell(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get woz data\n","        print(f\"loading woz data\")\n","        data_path = 'data/MultiWOZ_2.2'\n","        conversations.extend(self.get_conversations_woz(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        # Now get taskmaster data\n","        print(f\"loading taskmaster data\")\n","        data_path = 'data/Taskmaster'\n","        conversations.extend(self.get_conversations_taskmaster(data_path))\n","        print(f\"{len(conversations)=}\")\n","\n","        conversations_list = []\n","        dialogue_list = []\n","        for dialogue in  conversations:\n","            dialogue_list.append(\"<|startoftext|>\\n\")\n","            for line in dialogue:\n","                dialogue_list.append(line + '\\n')\n","            dialogue_list.append('<|endoftext|>\\n')\n","            conversations_list.append(''.join(dialogue_list))\n","            dialogue_list = []\n","        random.shuffle(conversations_list)\n","        text = ''.join(conversations_list)\n","        print(f\"{len(text)=}\")\n","\n","        # Tokenize the text into a sequence of input tokens\n","        tokenized_text = tokenizer.encode(text)\n","        print(f\"{len(tokenized_text)=}\")\n","\n","        # Split the tokenized text into chunks of max_length\n","        self.input_ids = []\n","        self.attn_masks = []\n","\n","        for i in range(0, len(tokenized_text) - max_length + 1, max_length):\n","            chunk = tokenized_text[i:i + max_length]\n","\n","            # Create padding if needed (only needed if chunks aren't exactly max_length)\n","            padding_length = max_length - len(chunk)\n","\n","            assert padding_length == 0, f\"Padding length should be 0, but got {padding_length}\"\n","\n","            # Pad input IDs with tokenizer's pad token id if necessary\n","            padded_chunk = chunk + [tokenizer.pad_token_id] * padding_length\n","\n","            # Create an attention mask: 1 for tokens, 0 for padding\n","            attention_mask = [1] * len(chunk) + [0] * padding_length\n","\n","            self.input_ids.append(padded_chunk)\n","            self.attn_masks.append(attention_mask)\n","        print(f\"Number of input sequences: {len(self.input_ids)}\")\n","\n","    def get_conversations_HF(self):\n","        train = []\n","        val = []\n","        test = []\n","        the_datasets = {'train': train, 'validation': val, 'test': test}\n","\n","        types = ['train', 'validation', 'test']\n","\n","        pattern = r'\\s([,.!?;:])'\n","        # Regular expressions for spaces around parentheses\n","        pattern_left_parenthesis = r'\\(\\s'\n","        pattern_right_parenthesis = r'\\s\\)'\n","\n","        for t in types:\n","            print(f\"loading: {t=}\")\n","            ds = dataset[t]\n","            # Access specific fields\n","            print(f\"{len(ds)=}\")\n","\n","            for i in range(len(ds)):\n","                dialogue = ds[i]['dialog']\n","                turn = [\"<|user|>\", \"<|assistant|>\"]\n","                conv = []\n","                for i, s in enumerate(dialogue):\n","                    s = s.strip()\n","                    s = normalize_text(s)\n","                    s = s.replace(\" ' \", \"'\")\n","                    s = s.replace('$ ', '$')\n","                    s = s.replace('( ', '(')\n","                    s = s.replace(') ', ')')\n","                    s = re.sub(pattern, r'\\1', s)\n","                    s = turn[0 if (i+1)%2 == 1 else 1] + s\n","                    conv.append(s)\n","                the_datasets[t].append(tuple(conv))\n","\n","        return the_datasets['train'] + the_datasets['validation'] + the_datasets['test']\n","\n","    def get_conversations_cornell(self, data_path):\n","        conversations = []\n","\n","        # Load movie lines\n","        id2line = {}\n","        with open(os.path.join(data_path, 'movie_lines.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 5:\n","                    line_id, text = parts[0], parts[4]\n","                    id2line[line_id] = text\n","\n","        # Load conversations\n","        with open(os.path.join(data_path, 'movie_conversations.txt'), 'r', encoding='iso-8859-1') as f:\n","            for line in f:\n","                parts = line.strip().split(' +++$+++ ')\n","                if len(parts) == 4:\n","                    conv_line_ids = ast.literal_eval(parts[3])  # safer than eval()\n","                    # Create pairs of conversations (input, response)\n","                    for i in range(len(conv_line_ids) - 1):\n","                        # Ensure both line IDs are in id2line\n","                        if conv_line_ids[i] in id2line and conv_line_ids[i + 1] in id2line:\n","                            input_line = \"<|user|>\" + normalize_text(id2line[conv_line_ids[i]])\n","                            response_line = \"<|assistant|>\" + normalize_text(id2line[conv_line_ids[i + 1]])\n","                            conversations.append((input_line, response_line))\n","                        #else:\n","                        #    print(f\"Missing line ID in conversation: {conv_line_ids[i]} or {conv_line_ids[i + 1]}\")\n","\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def get_conversations_woz(self, data_path):\n","        turn = ['<|user|>', '<|assistant|>']\n","        types = ['train', 'dev', 'test']\n","        the_datasets = {t : [] for t in types}\n","        dir = {t : os.path.join(data_path, t) for t in types}\n","        for t in types:\n","            print(f\"processing: {t=}\")\n","            json_files = [f for f in os.listdir(dir[t]) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            for json_file in json_files:\n","                file_path = os.path.join(dir[t], json_file)\n","                with open(file_path, 'r') as file:\n","                    dialogues.extend(json.load(file))\n","            for dialogue in dialogues:\n","                conversation_list = []\n","                conversation = dialogue['turns']\n","                for i, line in enumerate(conversation):\n","                    conversation_list.append(turn[0 if (i+1)%2 == 1 else 1] + normalize_text(line['utterance']))\n","                the_datasets[t].append(tuple(conversation_list))\n","            print(f\"{len(the_datasets[t])=}\")\n","\n","        return the_datasets['train'] + the_datasets['dev'] + the_datasets['test']\n","\n","    def get_conversations_taskmaster(self, data_path):\n","        dirs = ['TM-1-2019', 'TM-2-2020/data', 'TM-3-2020/data', 'TM-4-2024/data']\n","        conversations = []\n","        for dir in dirs:\n","            print(f\"{dir=}\")\n","            path = os.path.join(data_path, dir)\n","            json_files = [f for f in os.listdir(path) if f.endswith('.json')] # Filter the list to include only JSON files\n","            dialogues = [] # Initialize a list to store the data from all JSON files\n","            # Loop through each JSON file and load the data\n","            print(\"loading JSON files\")\n","            for json_file in json_files:\n","                file_path = os.path.join(path, json_file)\n","                with open(file_path, 'r') as file:\n","                    print(f\"{file_path=}\")\n","                    dialogues.extend(json.load(file))\n","            print(\"processing dialogues\")\n","            for dialogue in dialogues:\n","                utterances = dialogue['utterances']\n","                conversation_list = []\n","                previous_prompt = \"\"\n","                for line in utterances:\n","                    prompt = \"<|assistant|>\"\n","                    if line['speaker'].lower() == 'user':\n","                        prompt = \"<|user|>\"\n","                    if previous_prompt == prompt:\n","                        last_text = conversation_list.pop()\n","                        conversation_list.append(last_text + \" \" + normalize_text(line['text']))\n","                    else:\n","                        conversation_list.append(prompt + normalize_text(line['text']))\n","                        previous_prompt = prompt\n","                conversations.append(tuple(conversation_list))\n","        print(f\"{len(conversations)=}\")\n","\n","        return conversations\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.input_ids[idx]\n","        attn_mask = self.attn_masks[idx]\n","        return torch.tensor(input_ids), torch.tensor(attn_mask)\n","\n","\n","# Initialize dataset and dataloaders\n","dataset = ChatDataset(tokenizer, max_length=max_length)\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print(f'{train_size:,} training samples')\n","print(f'{val_size:,} validation samples')\n","\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n","validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","\n","# ========================================\n","#          Optimizer and Scheduler\n","# ========================================\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n","\n","# Calculate total steps\n","total_steps = len(train_dataloader) * epochs\n","\n","# Prepare learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",")\n","\n","# Initialize mixed precision training scaler with updated API\n","scaler = GradScaler()\n","\n","# ========================================\n","#          Training and Validation\n","# ========================================\n","\n","training_stats = []\n","initial_t0 = time.time()  # Measure total training time\n","\n","# Training loop\n","for epoch_i in range(epochs):\n","    print(f'\\n======== Epoch {epoch_i + 1} / {epochs} ========')\n","    print('Training...')\n","\n","    t0 = time.time()  # Measure epoch training time\n","    total_train_loss = 0  # Reset the total loss for this epoch\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","        model.zero_grad()  # Clear any previously calculated gradients\n","\n","        # Forward pass and compute loss with mixed precision\n","        with autocast(device_type = 'cuda', dtype=torch.float16):\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n","\n","        total_train_loss += loss.item()  # Accumulate the loss\n","\n","        scaler.scale(loss).backward()  # Backward pass with scaled loss\n","\n","        if (step + 1) % step_interval == 0:\n","            print(f\"step:{step+1}\")\n","\n","        # Update parameters every `gradient_accumulation_steps`\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            model.zero_grad()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)  # Calculate the average loss\n","    training_time = format_time(time.time() - t0)\n","\n","    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n","    print(f\"  Training epoch took: {training_time}\")\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    print(\"\\nRunning Validation...\")\n","\n","    t0 = time.time()  # Measure validation time\n","    total_eval_loss = 0\n","    model.eval()\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)\n","            loss = outputs.loss\n","\n","        total_eval_loss += loss.item()\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Validation took: {validation_time}\")\n","\n","    # Record all statistics from this epoch\n","    training_stats.append({\n","        'epoch': epoch_i + 1,\n","        'Training Loss': avg_train_loss,\n","        'Valid. Loss': avg_val_loss,\n","        'Training Time': training_time,\n","        'Validation Time': validation_time\n","    })\n","\n","    print(\"\\nEpoch Summary:\")\n","    print(f\"  Epoch {epoch_i + 1} / {epochs}\")\n","    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n","    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n","    print(f\"  Training Time: {training_time}\")\n","    print(f\"  Validation Time: {validation_time}\")\n","\n","    # ========================================\n","    #               Sampling\n","    # ========================================\n","    print(\"\\nGenerating Sample Output...\")\n","\n","    model.eval()\n","    sample_outputs = model.generate(\n","        bos_token_id=random.randint(1, 30000),\n","        do_sample=True,\n","        top_k=50,\n","        max_length=200,\n","        top_p=0.95,\n","        num_return_sequences=1\n","    )\n","    for i, sample_output in enumerate(sample_outputs):\n","        print(f\"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\")\n","    model.train()\n","\n","    # Save model checkpoint after each epoch\n","    if (epoch_i + 1) % checkpoint_interval == 0:\n","        checkpoint_dir = f'./checkpoint_chat_2-{epoch_i + 1}'\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        model.save_pretrained(checkpoint_dir)\n","        tokenizer.save_pretrained(checkpoint_dir)\n","\n","print(\"\\nTraining complete!\")\n","print(f\"Total training took {format_time(time.time() - initial_t0)} (h:mm:ss)\")\n","\n","# ========================================\n","#               Plotting\n","# ========================================\n","epochs = [x['epoch'] for x in training_stats]\n","training_loss = [x['Training Loss'] for x in training_stats]\n","validation_loss = [x['Valid. Loss'] for x in training_stats]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_loss, label='Training Loss')\n","plt.plot(epochs, validation_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss per Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1kIOcGLptAsC"},"source":["## SIMPLE EVAL ##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkbnkXmqrZCC"},"outputs":[],"source":["model.eval()\n","\n","# Prepare input prompt\n","prompt = \"<|startoftext|><|user|>To whome am I speaking?\\n<|assistant|>\"\n","generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n","\n","print(generated)\n","\n","# Generate text\n","sample_outputs = model.generate(\n","                                generated,\n","                                do_sample=True,\n","                                top_k=50,\n","                                max_length=300,\n","                                top_p=0.95,\n","                                num_return_sequences=1\n","                                )\n","\n","# Display generated text\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","\n","model.train()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"h3ErOZ2MJTJN","jupyter":{"outputs_hidden":false}},"source":["## Loading Fine-Tuned Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkc3PGkhJTJN"},"outputs":[],"source":["import os\n","import time\n","import datetime\n","import numpy as np\n","import random\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from torch.optim import AdamW  # Use PyTorch's AdamW optimizer\n","from torch.cuda.amp import GradScaler, autocast  # Updated imports for mixed precision training\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/gothic')\n","print(os.getcwd())\n","\n","# Enable CUDA Launch Blocking for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./checkpoint_chat_2-4\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./checkpoint_chat_2-4\")\n","model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings if needed\n","\n","# Set the pad token to the EOS token to avoid warnings\n","tokenizer.pad_token = tokenizer.eos_token\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cnVODli8JTJO","jupyter":{"outputs_hidden":false}},"source":["## Generate Text REPL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4nssaSVJTJM"},"outputs":[],"source":["def test():\n","    model.eval()  # Set model to evaluation mode\n","\n","    generated = '<|startoftext|>'\n","\n","    stop = ['<|user|>', '<|assistant|>', '<|endoftext|>', '<|startoftext|>']\n","\n","    while True:\n","        try:\n","            input_text = input(\"> \")\n","            if input_text == 'exit':\n","                print(\"-------------------------------------------\")\n","                print(generated)\n","                print(\"-------------------------------------------\")\n","                break\n","            initial_length = len(generated)\n","            generated += \"\\n<|user|>\" + input_text + \"\\n<|assistant|>\"\n","            new_length = len(generated)\n","            statement_length = new_length - initial_length\n","\n","            if new_length > 1024:\n","                generated = generated[-1024:]\n","                initial_length = 1024 - statement_length\n","                new_length = 1024\n","\n","            # Encode input text\n","            gg = torch.tensor(tokenizer.encode(generated)).unsqueeze(0).to(device)\n","\n","            # Create attention mask with correct shape\n","            attention_mask = torch.ones(gg.shape, dtype=torch.long, device=device)\n","\n","            # Generate output\n","            output = model.generate(\n","                gg,\n","                attention_mask=attention_mask,  # Pass the attention mask\n","                do_sample=True,\n","                top_k=50,\n","                max_length=1024,\n","                top_p=0.95,\n","                num_return_sequences=1,\n","                pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n","            )\n","\n","            generated = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","            for substring in stop:\n","                index = generated.find(substring, new_length)\n","                if index != -1:\n","                    generated = generated[:index]\n","\n","            print(generated[initial_length:])\n","\n","        except Exception as e:\n","            print(e)\n","\n","test()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}